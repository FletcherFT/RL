{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient Robert Miklos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.nn import relu, softmax\n",
    "from tensorflow.contrib.layers import fully_connected, convolution2d, flatten, batch_norm, max_pool2d, dropout\n",
    "import gym\n",
    "from gym import envs\n",
    "from utils import Viewer\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-27 11:15:27,210] Making new env: Pendulum-v0\n"
     ]
    }
   ],
   "source": [
    "# create gym environment\n",
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demo the environment\n",
    "env.reset() # reset the environment\n",
    "#view = Viewer(env, custom_render=True) # we use this custom viewer to render the environment inline in the notebook\n",
    "for timestep in range(200):\n",
    "#   #view.render()\n",
    "    env.render() # uncomment this to use gym's own render function\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action) # take a random action\n",
    "##view.render(close=True, display_gif=True) # display the environment inline in the notebook\n",
    "env.render(close=True) # uncomment this to use gym'm own render function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1)\n",
      "(?, 1)\n",
      "(?, 1)\n",
      "(?, 1)\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "n_actions = 1 # Power to the engine\n",
    "n_states = 3 # Position and velocity\n",
    "n_values = 1 # Number of value functions\n",
    "\n",
    "# Tuning the neural networks\n",
    "n_hidden_act = 100 # Number of neurons in the hidden layer by the actor\n",
    "n_hidden_cri = 100 # Number of neurons in the hidden layer by the critic\n",
    "n_hidden_act_t = n_hidden_act# Number of neurons in the hidden layer by the target actor\n",
    "n_hidden_cri_t = n_hidden_cri# Number of neurons in the hidden layer by the target critic\n",
    "\n",
    "\n",
    "# Defining the four neural networks: actor, critic, target actor, target critic.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "states_pl = tf.placeholder(tf.float32, [None, n_states], name='states_pl') # States of the environment\n",
    "actions_pl = tf.placeholder(tf.float32, [None, n_actions], name='actions_pl') # Action to the environment\n",
    "values_pl = tf.placeholder(tf.float32, [None, n_values], name='values_pl') # The action values\n",
    "sta_act_pl = tf.placeholder(tf.float32, [None, n_states+n_actions], name='sta_act_pl') # The concatenation of states and actions (for critic network)\n",
    "targets_pl = tf.placeholder(tf.float32, [None, n_values], name='targets_pl') # The target for the critic network\n",
    "learning_rate_pl = tf.placeholder(tf.float32, name='learning_rate_pl') # Learning rate\n",
    "\n",
    "scale = 2\n",
    "# The actor network\n",
    "l_hidden_act = tf.layers.dense(inputs=states_pl, units=n_hidden_act, activation=relu, name='l_hidden_act')\n",
    "l_out_act = tf.scalar_mul(scale,tf.layers.dense(inputs=l_hidden_act, units=n_actions, activation=tf.tanh, name='l_out_act') )\n",
    "\n",
    "# The target actor network\n",
    "l_hidden_act_t = tf.layers.dense(inputs=states_pl, units=n_hidden_act_t, activation=relu, name='tl_hidden_act')\n",
    "l_out_act_t = tf.scalar_mul(scale,tf.layers.dense(inputs=l_hidden_act_t, units=n_actions, activation=tf.tanh, name='tl_out_act') )\n",
    "\n",
    "# The critic network\n",
    "l_hidden_cri = tf.layers.dense(inputs=sta_act_pl, units=n_hidden_cri, activation=relu, name='l_hidden_cri',bias_initializer=tf.random_uniform_initializer)\n",
    "l_out_cri = tf.layers.dense(inputs=l_hidden_cri, units=n_values, activation=None, name='l_out_cri',bias_initializer=tf.random_uniform_initializer) \n",
    "\n",
    "# The target critic network\n",
    "l_hidden_cri_t = tf.layers.dense(inputs=sta_act_pl, units=n_hidden_cri_t, activation=relu, name='tl_hidden_cri')\n",
    "l_out_cri_t = tf.layers.dense(inputs=l_hidden_cri_t, units=n_values, activation=None, name='tl_out_cri') \n",
    "\n",
    "# The critic network for searching for the parameters of the actor network\n",
    "cl_hidden_cri = tf.layers.dense(inputs=tf.concat([states_pl,l_out_act],axis=1), units=n_hidden_cri, activation=relu, name='cl_hidden_cri',bias_initializer=tf.random_uniform_initializer)\n",
    "cl_out_cri = tf.layers.dense(inputs=cl_hidden_cri, units=n_values, activation=None, name='cl_out_cri',bias_initializer=tf.random_uniform_initializer) \n",
    "\n",
    "\n",
    "\n",
    "print(l_out_act.get_shape())\n",
    "print(l_out_act_t.get_shape())\n",
    "print(l_out_cri.get_shape())\n",
    "print(l_out_cri_t.get_shape())\n",
    "print(cl_out_cri.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.61711555 -0.78687254  0.97476273]\n",
      "[[ 0.15124236]]\n",
      "-\n",
      "[-0.61711555 -0.78687254  0.97476273]\n",
      "[[ 1.52156997]]\n",
      "-\n",
      "[-0.61711555 -0.78687254  0.97476273]\n",
      "[[ 1.46960318]]\n",
      "-------------------------------------------------------------------------\n",
      "[-0.69182262 -0.72206749 -0.42741768]\n",
      "[[ 0.04815253]]\n",
      "-\n",
      "[-0.69182262 -0.72206749 -0.42741768]\n",
      "[[ 0.11577675]]\n",
      "-\n",
      "[[ 0.14678001]\n",
      " [ 0.239768  ]\n",
      " [-0.00263113]\n",
      " [-0.10868609]\n",
      " [ 0.08241984]\n",
      " [-0.19100331]\n",
      " [ 0.03067139]\n",
      " [-0.00736395]\n",
      " [ 0.24042928]\n",
      " [-0.03593656]\n",
      " [-0.09122492]\n",
      " [-0.0340669 ]\n",
      " [-0.05000779]\n",
      " [ 0.10022172]\n",
      " [-0.04302855]\n",
      " [ 0.22534269]\n",
      " [-0.13738085]\n",
      " [ 0.11181656]\n",
      " [-0.12166858]\n",
      " [-0.03524382]\n",
      " [ 0.00488454]\n",
      " [-0.02487417]\n",
      " [-0.09112579]\n",
      " [ 0.07269648]\n",
      " [-0.0948287 ]\n",
      " [-0.23476003]\n",
      " [-0.02628875]\n",
      " [-0.07463999]\n",
      " [ 0.13870397]\n",
      " [-0.06921914]\n",
      " [ 0.08938485]\n",
      " [-0.07583109]\n",
      " [ 0.10872021]\n",
      " [ 0.23000601]\n",
      " [ 0.17011681]\n",
      " [ 0.0343768 ]\n",
      " [ 0.23463759]\n",
      " [ 0.12259427]\n",
      " [-0.16120903]\n",
      " [-0.05392572]\n",
      " [-0.03417191]\n",
      " [ 0.23447716]\n",
      " [-0.21879636]\n",
      " [-0.0711078 ]\n",
      " [ 0.16058597]\n",
      " [ 0.2421163 ]\n",
      " [ 0.11751896]\n",
      " [-0.00527847]\n",
      " [ 0.12902862]\n",
      " [ 0.22976178]\n",
      " [ 0.09979913]\n",
      " [ 0.24359307]\n",
      " [-0.19186282]\n",
      " [ 0.02302608]\n",
      " [-0.23066556]\n",
      " [ 0.11938745]\n",
      " [ 0.17539585]\n",
      " [-0.10398914]\n",
      " [-0.16739106]\n",
      " [ 0.01225719]\n",
      " [-0.09752637]\n",
      " [ 0.24238053]\n",
      " [ 0.23566335]\n",
      " [ 0.23517907]\n",
      " [ 0.03885376]\n",
      " [ 0.07917187]\n",
      " [-0.06824917]\n",
      " [ 0.03057116]\n",
      " [-0.06063023]\n",
      " [-0.02582364]\n",
      " [-0.21003823]\n",
      " [-0.17450827]\n",
      " [ 0.0593715 ]\n",
      " [ 0.12256742]\n",
      " [ 0.22316679]\n",
      " [ 0.1423696 ]\n",
      " [-0.12195849]\n",
      " [-0.01817262]\n",
      " [ 0.07732895]\n",
      " [-0.22321433]\n",
      " [-0.23470809]\n",
      " [-0.070195  ]\n",
      " [-0.06448919]\n",
      " [-0.14331639]\n",
      " [-0.05712338]\n",
      " [ 0.05137339]\n",
      " [ 0.21700713]\n",
      " [ 0.0699003 ]\n",
      " [-0.05234756]\n",
      " [-0.13041224]\n",
      " [ 0.06679374]\n",
      " [ 0.02492791]\n",
      " [-0.09814681]\n",
      " [ 0.10704169]\n",
      " [ 0.22798377]\n",
      " [-0.13553533]\n",
      " [ 0.17371699]\n",
      " [ 0.06067696]\n",
      " [ 0.15432927]\n",
      " [ 0.15626615]]\n",
      "[[-0.14020723 -0.1611872  -0.21030863 -0.02811489 -0.02447127 -0.087257\n",
      "   0.09325957 -0.12404972 -0.00057055 -0.22506352 -0.18166718 -0.05746627\n",
      "   0.18659529  0.12074706 -0.19401407  0.1419183  -0.03049871  0.06130043\n",
      "  -0.00821233  0.1581147   0.07096303 -0.15448585 -0.0930292   0.0880937\n",
      "  -0.11230499  0.20451248  0.07020035 -0.23335281 -0.2112349  -0.13555732\n",
      "   0.00749066 -0.04885708  0.08999959  0.12396628 -0.20196801  0.21351191\n",
      "   0.13778287 -0.22447476  0.16276312  0.03065351 -0.10386352  0.15499666\n",
      "   0.0447433  -0.1387175   0.21512908  0.08141038  0.06554201 -0.22748846\n",
      "  -0.11877653  0.02865589 -0.12510383  0.01400068 -0.07032146  0.16112107\n",
      "   0.02965862  0.23957404  0.15193355 -0.03174259  0.20849001 -0.21520089\n",
      "  -0.11107107 -0.01169275  0.12669429 -0.00288787 -0.22395706  0.08468986\n",
      "  -0.04044826 -0.10941435  0.15566388 -0.22336636  0.00133534 -0.11051169\n",
      "   0.22861174 -0.05015668  0.22258067  0.11339876  0.05359295  0.18155029\n",
      "  -0.02842653  0.02601951 -0.15419865  0.04833499  0.03953114  0.0544399\n",
      "   0.03880501  0.22920683  0.21335313  0.21464053  0.23119524 -0.20884627\n",
      "   0.14731908 -0.05409478  0.23356012  0.04274532  0.18589503 -0.0649118\n",
      "   0.12604964  0.22416854 -0.11551309 -0.00470631]\n",
      " [ 0.14571607 -0.14394042  0.03124771 -0.07703324  0.1088962  -0.12134034\n",
      "  -0.02595314  0.07898733 -0.00867625 -0.02030669 -0.0270049   0.22335103\n",
      "   0.21421093  0.11031333  0.08403146  0.07107165  0.20210198 -0.21342111\n",
      "  -0.21673201 -0.15805067  0.10279831 -0.08115029  0.01192003  0.01293868\n",
      "  -0.21172899  0.07571673 -0.19510557 -0.1422013   0.21252665  0.04626182\n",
      "  -0.21381131  0.2374289   0.05364558  0.03043172  0.03542814  0.05620262\n",
      "  -0.17913538  0.16800234 -0.02365087  0.10986161 -0.14741448 -0.17628407\n",
      "  -0.14680919 -0.07302929  0.07610032 -0.06909968 -0.09110878  0.14466834\n",
      "   0.13931897 -0.23891485  0.14154911 -0.02244964 -0.09110059  0.04149067\n",
      "  -0.0364646   0.0866707   0.05934179 -0.17474477  0.05077893 -0.00731599\n",
      "   0.03384101  0.17598841  0.00942752 -0.05007714  0.04206777 -0.1648311\n",
      "   0.05987695  0.07319245  0.2063739   0.22876847  0.06179222 -0.06006879\n",
      "   0.17038834  0.18341827  0.18368101 -0.20291348  0.16352853 -0.04278806\n",
      "  -0.00162785 -0.083975   -0.08770241  0.07707354 -0.16674918  0.12105635\n",
      "  -0.13987927 -0.04866736  0.2283487  -0.23459405 -0.09339696 -0.14352146\n",
      "   0.12513894  0.02410293  0.08758283 -0.02340467 -0.13744873  0.13275346\n",
      "   0.06168821 -0.17821866  0.0901269   0.02126035]\n",
      " [ 0.01720548 -0.10805708  0.07824343  0.20218185  0.07634157  0.20917371\n",
      "  -0.00051333  0.10041329 -0.20050487 -0.1419774   0.09194395  0.10770363\n",
      "  -0.19420941 -0.03724863  0.01849702  0.00119309 -0.20500822  0.20070192\n",
      "  -0.01746783  0.19704697 -0.16608003  0.12675503  0.03173664 -0.11845366\n",
      "  -0.22399499 -0.1765829   0.01405892  0.08713099  0.11631757 -0.21154059\n",
      "   0.13361347 -0.13470834 -0.07131995  0.16497651 -0.18938369 -0.22569349\n",
      "  -0.08159181 -0.04980306 -0.09646553  0.02252266 -0.02024753 -0.12914333\n",
      "  -0.15615292  0.00464819  0.15002587  0.04680026 -0.00646026 -0.1944142\n",
      "   0.23331249 -0.04325683 -0.05158782  0.22587824  0.03187722 -0.10807908\n",
      "   0.12148827 -0.12346761  0.14929792  0.06950337 -0.14430681 -0.1554947\n",
      "   0.17364296  0.1029326   0.19544196 -0.11033736 -0.23608172 -0.13590895\n",
      "  -0.23338659 -0.22633597  0.23979658  0.17241237 -0.15108034 -0.18284611\n",
      "   0.06824818 -0.05177091 -0.1149465   0.02956143 -0.10792531 -0.05415845\n",
      "   0.00439422 -0.10315199 -0.17100814  0.21057501  0.02931422 -0.16066247\n",
      "   0.01081783 -0.01741388  0.05427316 -0.12683664  0.11902857 -0.09066547\n",
      "  -0.21784796 -0.1132213  -0.05797142 -0.07575482  0.11175272  0.19283158\n",
      "  -0.06032002  0.00726858 -0.18047374  0.0143882 ]\n",
      " [-0.10107769 -0.04171345  0.01075307  0.11046541  0.18966636  0.10818788\n",
      "   0.06499186 -0.21323052 -0.09776026  0.00246365 -0.00822419  0.15112609\n",
      "   0.22876373 -0.13642818 -0.15823014  0.21982753  0.13637522  0.07258034\n",
      "   0.05008847  0.12883031 -0.19660217  0.08208692 -0.06300266 -0.23705502\n",
      "  -0.11599419  0.12200519 -0.10535835  0.13199812  0.07721719  0.0957939\n",
      "   0.11949253 -0.00505547  0.10001639  0.06954694 -0.00201772 -0.18670723\n",
      "   0.11773738  0.07070178 -0.18315352 -0.20957333  0.19891846  0.06043866\n",
      "   0.03954095  0.09391722  0.11371201  0.14188805  0.14885709  0.23722351\n",
      "   0.02014959 -0.03396395  0.20593598  0.0107379  -0.18510635 -0.00387509\n",
      "   0.20935038  0.0856871  -0.12243018 -0.16300094  0.16121617 -0.09835365\n",
      "  -0.03906289 -0.22286558 -0.07961412  0.17716101  0.10856628  0.01070678\n",
      "  -0.07110927 -0.11465703 -0.12955675  0.01162186 -0.06296486 -0.06127854\n",
      "   0.04168156  0.16021076 -0.05011036 -0.21589461 -0.15029813  0.2266227\n",
      "  -0.17094246  0.1557346   0.0094676  -0.12149215 -0.22469082 -0.1673955\n",
      "  -0.12330784  0.17923552  0.18750849  0.22495487  0.14051247 -0.0273253\n",
      "  -0.04086305 -0.04809062 -0.21677038  0.14528835  0.18852538  0.0956248\n",
      "  -0.05142199  0.21863091  0.01285222 -0.00846179]]\n",
      "110011\n",
      "[ 0.91837919]\n",
      "[ 0.70440125  0.08667624  0.68578017  0.44462168  0.24610734  0.16863275\n",
      "  0.52954304  0.96576941  0.80639577  0.45340669  0.40767717  0.62618458\n",
      "  0.68321681  0.57287502  0.96268928  0.04000521  0.81163967  0.95230758\n",
      "  0.58811879  0.83648074  0.82992268  0.41849935  0.08925259  0.22140312\n",
      "  0.61435568  0.11290884  0.53531408  0.6166265   0.26655209  0.58260441\n",
      "  0.19938898  0.5236057   0.96520245  0.02468705  0.44178581  0.95686615\n",
      "  0.39144397  0.93347824  0.10815299  0.29325843  0.92535031  0.47628152\n",
      "  0.89755476  0.83221829  0.82078671  0.10860932  0.46470273  0.38469207\n",
      "  0.1381582   0.53695238  0.96231723  0.73154771  0.84715021  0.83484733\n",
      "  0.92672122  0.03795433  0.57859242  0.72130442  0.59750831  0.35268641\n",
      "  0.77188897  0.77098429  0.05328643  0.27758479  0.13758099  0.39134157\n",
      "  0.18861842  0.71358001  0.65698457  0.01994455  0.43654573  0.11770391\n",
      "  0.34327626  0.1923418   0.85742307  0.63576353  0.89608693  0.7215085\n",
      "  0.91977179  0.43442202  0.13848817  0.16437018  0.13636971  0.9673264\n",
      "  0.79652703  0.31376278  0.01775134  0.25691473  0.39530623  0.54748547\n",
      "  0.11746061  0.69121468  0.98162651  0.70340717  0.71612942  0.37155759\n",
      "  0.81226301  0.29277658  0.78573465  0.08142114]\n",
      "110011\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# test forward pass of actor network\n",
    "state = env.reset()\n",
    "#yy = ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES,'l_out_act')\n",
    "#uu = tf.get_variable(yy[1])\n",
    "\n",
    "# Test gradient function\n",
    "w = tf.placeholder(tf.float32, name='w');\n",
    "u = tf.placeholder(tf.float32, name='u');\n",
    "f = w*u\n",
    "g = tf.gradients(l_out_cri,sta_act_pl);\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_pl)\n",
    "#g1 = optimizer.compute_gradients(l_out_act,[ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES,'l_hidden_act'),ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES,'l_out_act')])\n",
    "#g1 = optimizer.compute_gradients(l_out_cri,sta_act_pl)\n",
    "\n",
    "\n",
    "    \n",
    "x_values = np.zeros((10,1))\n",
    "x_var = tf.Variable(x_values, name=\"xVariable\", dtype=tf.float32)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #sess.run(copy_criact_critic)\n",
    "    \n",
    "    state = env.reset()\n",
    "    action_totake1 = sess.run(fetches=l_out_act, feed_dict={states_pl: np.reshape(state,(1,n_states))})\n",
    "    print(state)\n",
    "    print(action_totake1)\n",
    "\n",
    "    print('-')\n",
    "\n",
    "# test forward pass of critic network\n",
    "    #state = env.reset()\n",
    "    action = (np.random.rand()-0.5)*2; # Uniform distribution between -1 and 1\n",
    "    action = action_totake1\n",
    "#with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    action_totake = sess.run(fetches=l_out_cri, feed_dict={sta_act_pl: np.column_stack((np.reshape(state,(1,n_states)),action))})\n",
    "    print(state)\n",
    "    #print(action)\n",
    "    print(action_totake)\n",
    "    print('-')\n",
    "    \n",
    "    action_totake = sess.run(fetches=cl_out_cri, feed_dict={states_pl: np.reshape(state,(1,n_states))})\n",
    "    print(state)\n",
    "    print(action_totake)\n",
    "    print('-------------------------------------------------------------------------')\n",
    "    \n",
    "\n",
    "\n",
    "# test forward pass of targetactor network\n",
    "    state = env.reset()\n",
    "#with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    value = sess.run(fetches=l_out_act_t, feed_dict={states_pl: np.reshape(state,(1,n_states))})\n",
    "    print(state)\n",
    "    print(value)\n",
    "    print('-')\n",
    "\n",
    "# test forward pass of target critic network\n",
    "    #state = env.reset()\n",
    "    #action = (np.random.rand()-0.5)*2; # Uniform distribution between -1 and 1\n",
    "#with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    value = sess.run(fetches=l_out_cri_t, feed_dict={sta_act_pl: np.column_stack((np.reshape(state,(1,n_states)),action))})\n",
    "    print(state)\n",
    "    #print(action)\n",
    "    print(value)\n",
    "    print('-')\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "  #  print(x_var.eval())\n",
    "    aa = ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES,'l_out_cri')\n",
    "    bb = ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES,'l_hidden_cri')\n",
    "    #print(aa)\n",
    "    #print(g)\n",
    "    for i in range(len(aa)):\n",
    "        print(aa[i].eval())\n",
    "        print(bb[i].eval())\n",
    "        print('110011')\n",
    "    \n",
    "    print('---------------')\n",
    "    i = 0\n",
    "    #print('--')\n",
    "    #print(aa[i])\n",
    "    #print(aa[i].eval())\n",
    "    #print(np.shape(aa[i].eval()))\n",
    "    #aa[i] = aa[i].assign(x_var+1)\n",
    "    #print(aa[i].eval())\n",
    "    \n",
    "    #print('--')\n",
    "    #ans = sess.run(fetches=f, feed_dict={u: 2,w:4})\n",
    "    #print(ans)\n",
    "    #ans = sess.run(fetches=g, feed_dict={u: 2,w:4})\n",
    "    #print(ans)\n",
    "    #print('--')\n",
    "    #print()\n",
    "#    print(g)\n",
    "    #print(g1)\n",
    "    #print(len(g1))\n",
    "    #aa = []\n",
    "    #for i in range(len(g1)):\n",
    "        #aa = g[i].eval(feed_dict={sta_act_pl: np.reshape([1,1,1],(1,3))})\n",
    "        #print(aa)\n",
    "        #print(aa[0][-1])\n",
    "        #print(g[i].eval(feed_dict={sta_act_pl: np.reshape([0,0,1],(1,3))}))\n",
    "    #    bb = g1[i][0].eval(feed_dict={states_pl: np.reshape([1,1],(1,2))})\n",
    "    #    print(bb)\n",
    "        #aa = np.concatenate(aa,bb)\n",
    "        #print(g1[i][0].eval(feed_dict={states_pl: np.reshape([0,0],(1,2))}))\n",
    "    #print(bb[:])\n",
    "    #ff = ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES,'l_out_cri')\n",
    "    #print(ff)\n",
    "    #print(g)\n",
    "    #for i in range(len(ff)):\n",
    "    #    print('..')\n",
    "    #    print(ff[i].eval())\n",
    "    #    ss = ff[i].get_shape()\n",
    "    #    ze = np.zeros((ss[0],1))\n",
    "    #    ff[i] = ff[i].assign(ze)\n",
    "    #    aa = ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES,'l_out_cri')\n",
    "    #    print(ff[i].eval())\n",
    "    #    print(aa[i].eval())\n",
    "        \n",
    "        \n",
    "    \n",
    "    #print(ff)\n",
    "    #ze = np.zeros((len(ff),1))\n",
    "    #ff = ff.assign(ze)\n",
    "    #print(ff)\n",
    "    #print(g)\n",
    "    #print(g1[3])\n",
    "    #print(g.eval(feed_dict={sta_act_pl: np.reshape([1,1,1],(1,3))}))\n",
    "    #print(g1[1][0].eval(feed_dict={sta_act_pl: np.reshape([1,1,1],(1,3))}))\n",
    "    #g_f = sess.run(fetches=g, feed_dict={sta_act_pl: np.reshape([1,1,1],(1,3))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mul:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"l_hidden_cri/Relu:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define Q-value loss function\n",
    "loss_f = tf.reduce_mean(tf.square(targets_pl-l_out_cri))\n",
    "#loss_f = (tf.square(targets_pl-l_out_cri))\n",
    "\n",
    "#Regularization\n",
    "#reg_scale = 0.001\n",
    "#regularize = tf.contrib.layers.l2_regularizer(reg_scale)\n",
    "#params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'l_out_cri')\n",
    "#reg_term = sum([regularize(param) for param in params])\n",
    "#loss_f += reg_term\n",
    "\n",
    "# Maximize the reward (maximize the output of the critic network)\n",
    "in_to_cri = l_out_act\n",
    "#print([states_pl, in_to_cri])\n",
    "print(in_to_cri)\n",
    "print(l_hidden_cri)\n",
    "#exReward = l_out_cri(feed_dict={sta_act_pl: [states_pl, in_to_cri]})\n",
    "\n",
    "\n",
    "# Gradient of the critic network wrt to the action\n",
    "#g_C = tf.gradients(l_out_cri,sta_act_pl)\n",
    "#print(g_C)\n",
    "\n",
    "# Gradient of the actor network wrt its own parameters\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_pl)\n",
    "#g_A = optimizer.compute_gradients(l_out_act,[tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'l_hidden_act'),tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'l_out_act')])\n",
    "#print(g_A)\n",
    "\n",
    "#Minus sign due to converting a maximization to a minimization problem\n",
    "sign = -1\n",
    "loss_c = tf.reduce_mean(tf.scalar_mul(sign,cl_out_cri))\n",
    "g_c = optimizer.compute_gradients(loss_c,[tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'l_hidden_act'),tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'l_out_act')])\n",
    "train_c = optimizer.apply_gradients(g_c)\n",
    "#train_c = optimizer.minimize(cl_out_cri,aa)\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_pl)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_pl)\n",
    "train_f = optimizer.minimize(loss_f)\n",
    "\n",
    "saver = tf.train.Saver() # we use this later to save the model\n",
    "print(loss_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Additional functions\n",
    "\n",
    "# Update the target network's parameters\n",
    "tau = 0.5\n",
    "def assign_trainables(t_p, p, tau=1.0):\n",
    "    \"\"\"Update trainable variables with rate tau.\"\"\"\n",
    "    obs = []\n",
    "    for i, t in enumerate(t_p):\n",
    "        for k in range(len(t)):\n",
    "            obs.append(t[k].assign((1-tau) * t[k].value() + tau * p[i][k].value()))\n",
    "    return obs\n",
    "\n",
    "\n",
    "# Get the target actor parameters\n",
    "pa_t_h = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='tl_hidden_act') #The hidden layer\n",
    "pa_t_o = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='tl_out_act')    #The output layer\n",
    "pa_t = [pa_t_h,pa_t_o]\n",
    "\n",
    "# Get the actor parameters\n",
    "pa_h = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='l_hidden_act') #The hidden layer\n",
    "pa_o = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='l_out_act')    #The output layer\n",
    "pa = [pa_h,pa_o];\n",
    "\n",
    "# Get the target critic parameters\n",
    "pc_t_h = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='tl_hidden_cri') #The hidden layer\n",
    "pc_t_o = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='tl_out_cri')    #The output layer\n",
    "pc_t = [pc_t_h,pc_t_o]\n",
    "\n",
    "# Get the critic parameters\n",
    "pc_h = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='l_hidden_cri') #The hidden layer\n",
    "pc_o = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='l_out_cri')    #The output layer\n",
    "pc = [pc_h,pc_o];\n",
    "\n",
    "# Get the critic-actor parameters\n",
    "p_h_c = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='cl_hidden_cri') #The hidden layer\n",
    "p_o_c = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='cl_out_cri')    #The output layer\n",
    "p = [p_h_c,p_o_c];\n",
    "\n",
    "update_target_actor = assign_trainables( pa_t,pa,tau)\n",
    "update_target_critic = assign_trainables( pc_t,pc,tau)\n",
    "copy_target_actor = assign_trainables( pa_t,pa,tau=1) #copy with tau=1 (clever trick)\n",
    "copy_target_critic = assign_trainables( pc_t,pc,tau=1)\n",
    "copy_critic_criact = assign_trainables( pc,p,tau=1)\n",
    "copy_criact_critic = assign_trainables( p,pc,tau=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Noise is changed\n",
      "0.05\n",
      "Episode:    0. Mean training reward: -103.68. Validation reward: -1081.85. Mean loss target:  31.82. Mean loss:  35.31.\n",
      "Episode:   10. Mean training reward: -1370.99. Validation reward: -940.75. Mean loss target:  69.97. Mean loss: 108.99.\n",
      "Episode:   20. Mean training reward: -1085.05. Validation reward: -754.26. Mean loss target:  65.44. Mean loss: 106.08.\n",
      "Episode:   30. Mean training reward: -879.14. Validation reward: -1011.64. Mean loss target:  57.31. Mean loss: 100.08.\n",
      "Episode:   40. Mean training reward: -762.43. Validation reward: -815.74. Mean loss target:  52.16. Mean loss:  96.61.\n",
      "Episode:   50. Mean training reward: -856.78. Validation reward: -519.47. Mean loss target:  49.17. Mean loss:  93.86.\n",
      "Episode:   60. Mean training reward: -732.70. Validation reward: -968.82. Mean loss target:  46.89. Mean loss:  91.53.\n",
      "Episode:   70. Mean training reward: -687.02. Validation reward: -1015.75. Mean loss target:  44.51. Mean loss:  89.46.\n",
      "Episode:   80. Mean training reward: -831.95. Validation reward: -1044.28. Mean loss target:  42.46. Mean loss:  87.52.\n",
      "Episode:   90. Mean training reward: -755.77. Validation reward: -760.63. Mean loss target:  40.92. Mean loss:  85.84.\n",
      "Episode:  100. Mean training reward: -748.12. Validation reward: -1212.13. Mean loss target:  40.00. Mean loss:  84.39.\n",
      "Episode:  110. Mean training reward: -833.52. Validation reward: -657.22. Mean loss target:  39.00. Mean loss:  83.17.\n",
      "Episode:  120. Mean training reward: -695.12. Validation reward: -636.35. Mean loss target:  37.90. Mean loss:  82.06.\n",
      "Episode:  130. Mean training reward: -625.92. Validation reward: -768.61. Mean loss target:  36.90. Mean loss:  80.97.\n",
      "Episode:  140. Mean training reward: -545.56. Validation reward: -517.42. Mean loss target:  35.84. Mean loss:  79.88.\n",
      "Episode:  150. Mean training reward: -493.78. Validation reward: -525.71. Mean loss target:  34.98. Mean loss:  78.73.\n",
      "Episode:  160. Mean training reward: -420.96. Validation reward: -575.59. Mean loss target:  34.01. Mean loss:  77.51.\n",
      "Episode:  170. Mean training reward: -248.53. Validation reward: -133.46. Mean loss target:  33.17. Mean loss:  76.23.\n",
      "Episode:  180. Mean training reward: -308.62. Validation reward: -1085.80. Mean loss target:  32.31. Mean loss:  75.01.\n",
      "Episode:  190. Mean training reward: -439.83. Validation reward: -128.31. Mean loss target:  31.65. Mean loss:  73.74.\n",
      "Episode:  200. Mean training reward: -396.40. Validation reward: -129.64. Mean loss target:  31.02. Mean loss:  72.52.\n",
      "Episode:  210. Mean training reward: -228.80. Validation reward: -798.83. Mean loss target:  30.47. Mean loss:  71.35.\n",
      "Episode:  220. Mean training reward: -193.34. Validation reward: -267.14. Mean loss target:  29.92. Mean loss:  70.19.\n",
      "Episode:  230. Mean training reward: -335.32. Validation reward: -1073.94. Mean loss target:  29.39. Mean loss:  69.04.\n",
      "Episode:  240. Mean training reward: -310.25. Validation reward: -352.82. Mean loss target:  28.82. Mean loss:  67.98.\n",
      "Episode:  250. Mean training reward: -196.14. Validation reward: -11.54. Mean loss target:  28.37. Mean loss:  66.93.\n",
      "Episode:  260. Mean training reward: -277.26. Validation reward: -720.51. Mean loss target:  27.87. Mean loss:  65.92.\n",
      "Episode:  270. Mean training reward: -645.81. Validation reward: -131.98. Mean loss target:  27.44. Mean loss:  64.91.\n",
      "Episode:  280. Mean training reward: -302.66. Validation reward: -633.71. Mean loss target:  27.07. Mean loss:  64.03.\n",
      "Episode:  290. Mean training reward: -239.60. Validation reward: -466.20. Mean loss target:  26.67. Mean loss:  63.16.\n",
      "Episode:  300. Mean training reward: -312.87. Validation reward: -520.75. Mean loss target:  26.27. Mean loss:  62.31.\n",
      "Episode:  310. Mean training reward: -364.77. Validation reward: -411.70. Mean loss target:  25.84. Mean loss:  61.49.\n",
      "Episode:  320. Mean training reward: -457.17. Validation reward: -645.61. Mean loss target:  25.42. Mean loss:  60.62.\n",
      "Episode:  330. Mean training reward: -328.73. Validation reward: -524.30. Mean loss target:  25.04. Mean loss:  59.78.\n",
      "Episode:  340. Mean training reward: -440.03. Validation reward: -393.44. Mean loss target:  24.63. Mean loss:  59.00.\n",
      "Episode:  350. Mean training reward: -454.10. Validation reward: -362.93. Mean loss target:  24.24. Mean loss:  58.27.\n",
      "Episode:  360. Mean training reward: -296.42. Validation reward: -125.75. Mean loss target:  23.90. Mean loss:  57.52.\n",
      "Episode:  370. Mean training reward: -181.07. Validation reward: -123.01. Mean loss target:  23.59. Mean loss:  56.82.\n",
      "Episode:  380. Mean training reward: -307.37. Validation reward: -628.36. Mean loss target:  23.24. Mean loss:  56.10.\n",
      "Episode:  390. Mean training reward: -244.06. Validation reward: -757.78. Mean loss target:  22.92. Mean loss:  55.37.\n",
      "Episode:  400. Mean training reward: -337.94. Validation reward: -129.02. Mean loss target:  22.66. Mean loss:  54.63.\n",
      "Episode:  410. Mean training reward: -235.68. Validation reward: -127.93. Mean loss target:  22.39. Mean loss:  53.94.\n",
      "Episode:  420. Mean training reward: -411.97. Validation reward: -619.55. Mean loss target:  22.20. Mean loss:  53.19.\n",
      "Episode:  430. Mean training reward: -375.11. Validation reward: -652.50. Mean loss target:  21.97. Mean loss:  52.45.\n",
      "Episode:  440. Mean training reward: -383.88. Validation reward: -263.64. Mean loss target:  21.73. Mean loss:  51.74.\n",
      "Episode:  450. Mean training reward: -327.06. Validation reward: -657.96. Mean loss target:  21.52. Mean loss:  51.05.\n",
      "Episode:  460. Mean training reward: -365.50. Validation reward: -757.19. Mean loss target:  21.34. Mean loss:  50.36.\n",
      "Episode:  470. Mean training reward: -465.24. Validation reward: -737.49. Mean loss target:  21.18. Mean loss:  49.68.\n",
      "Episode:  480. Mean training reward: -341.40. Validation reward: -118.69. Mean loss target:  20.98. Mean loss:  49.02.\n",
      "Episode:  490. Mean training reward: -481.61. Validation reward: -125.92. Mean loss target:  20.82. Mean loss:  48.36.\n",
      "Episode:  500. Mean training reward: -368.45. Validation reward: -250.11. Mean loss target:  20.70. Mean loss:  47.69.\n",
      "Episode:  510. Mean training reward: -341.23. Validation reward: -243.56. Mean loss target:  20.56. Mean loss:  47.02.\n",
      "Episode:  520. Mean training reward: -328.95. Validation reward: -253.82. Mean loss target:  20.42. Mean loss:  46.35.\n",
      "Episode:  530. Mean training reward: -701.48. Validation reward: -291.99. Mean loss target:  20.33. Mean loss:  45.67.\n",
      "Episode:  540. Mean training reward: -756.34. Validation reward: -1027.06. Mean loss target:  20.31. Mean loss:  45.02.\n",
      "Episode:  550. Mean training reward: -525.09. Validation reward: -124.87. Mean loss target:  20.21. Mean loss:  44.45.\n",
      "Episode:  560. Mean training reward: -186.50. Validation reward: -120.96. Mean loss target:  20.02. Mean loss:  43.94.\n",
      "Episode:  570. Mean training reward: -398.67. Validation reward: -1161.74. Mean loss target:  19.92. Mean loss:  43.32.\n",
      "Episode:  580. Mean training reward: -525.28. Validation reward: -236.35. Mean loss target:  19.91. Mean loss:  42.88.\n",
      "Episode:  590. Mean training reward: -493.78. Validation reward: -242.47. Mean loss target:  19.77. Mean loss:  42.34.\n",
      "Episode:  600. Mean training reward: -301.23. Validation reward: -131.21. Mean loss target:  19.63. Mean loss:  41.81.\n",
      "Episode:  610. Mean training reward: -929.29. Validation reward: -1642.10. Mean loss target:  19.77. Mean loss:  41.28.\n",
      "Episode:  620. Mean training reward: -1401.69. Validation reward: -1244.32. Mean loss target:  19.74. Mean loss:  40.92.\n",
      "Episode:  630. Mean training reward: -1204.22. Validation reward: -1135.80. Mean loss target:  19.60. Mean loss:  40.57.\n",
      "Episode:  640. Mean training reward: -1025.90. Validation reward: -713.83. Mean loss target:  19.48. Mean loss:  40.35.\n",
      "Episode:  650. Mean training reward: -899.45. Validation reward: -864.03. Mean loss target:  19.37. Mean loss:  40.27.\n",
      "Episode:  660. Mean training reward: -702.85. Validation reward: -617.59. Mean loss target:  19.27. Mean loss:  40.21.\n",
      "Episode:  670. Mean training reward: -543.63. Validation reward: -252.28. Mean loss target:  19.16. Mean loss:  40.05.\n",
      "Episode:  680. Mean training reward: -354.05. Validation reward: -241.48. Mean loss target:  19.05. Mean loss:  39.86.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  690. Mean training reward: -309.91. Validation reward: -416.11. Mean loss target:  18.93. Mean loss:  39.68.\n",
      "Episode:  700. Mean training reward: -217.06. Validation reward: -119.87. Mean loss target:  18.79. Mean loss:  39.50.\n",
      "Episode:  710. Mean training reward: -233.46. Validation reward: -118.33. Mean loss target:  18.66. Mean loss:  39.30.\n",
      "Episode:  720. Mean training reward: -208.87. Validation reward: -566.68. Mean loss target:  18.54. Mean loss:  39.11.\n",
      "Episode:  730. Mean training reward: -220.10. Validation reward: -244.14. Mean loss target:  18.41. Mean loss:  38.92.\n",
      "Episode:  740. Mean training reward: -260.35. Validation reward: -130.34. Mean loss target:  18.31. Mean loss:  38.69.\n",
      "Episode:  750. Mean training reward: -182.75. Validation reward: -125.54. Mean loss target:  18.20. Mean loss:  38.48.\n",
      "Episode:  760. Mean training reward: -158.82. Validation reward:  -3.53. Mean loss target:  18.09. Mean loss:  38.30.\n",
      "Episode:  770. Mean training reward: -182.09. Validation reward: -122.27. Mean loss target:  17.95. Mean loss:  38.11.\n",
      "Episode:  780. Mean training reward: -196.09. Validation reward: -300.48. Mean loss target:  17.82. Mean loss:  37.92.\n",
      "Episode:  790. Mean training reward: -156.36. Validation reward:  -1.22. Mean loss target:  17.69. Mean loss:  37.74.\n",
      "Episode:  800. Mean training reward: -168.34. Validation reward: -123.74. Mean loss target:  17.57. Mean loss:  37.54.\n",
      "Episode:  810. Mean training reward: -194.55. Validation reward: -126.72. Mean loss target:  17.45. Mean loss:  37.33.\n",
      "Episode:  820. Mean training reward: -143.47. Validation reward: -231.89. Mean loss target:  17.34. Mean loss:  37.09.\n",
      "Episode:  830. Mean training reward: -112.17. Validation reward: -115.94. Mean loss target:  17.25. Mean loss:  36.85.\n",
      "Episode:  840. Mean training reward: -199.75. Validation reward: -125.72. Mean loss target:  17.15. Mean loss:  36.61.\n",
      "Episode:  850. Mean training reward: -157.36. Validation reward: -126.30. Mean loss target:  17.06. Mean loss:  36.34.\n",
      "Episode:  860. Mean training reward: -136.09. Validation reward: -114.74. Mean loss target:  16.96. Mean loss:  36.08.\n",
      "Episode:  870. Mean training reward: -244.39. Validation reward: -304.56. Mean loss target:  16.87. Mean loss:  35.81.\n",
      "Episode:  880. Mean training reward: -198.42. Validation reward: -332.16. Mean loss target:  16.77. Mean loss:  35.56.\n",
      "Episode:  890. Mean training reward: -190.22. Validation reward: -127.47. Mean loss target:  16.70. Mean loss:  35.27.\n",
      "Episode:  900. Mean training reward: -151.44. Validation reward:  -1.24. Mean loss target:  16.64. Mean loss:  34.99.\n",
      "Episode:  910. Mean training reward: -174.52. Validation reward: -130.50. Mean loss target:  16.56. Mean loss:  34.74.\n",
      "Episode:  920. Mean training reward: -172.46. Validation reward: -127.95. Mean loss target:  16.49. Mean loss:  34.44.\n",
      "Episode:  930. Mean training reward: -198.67. Validation reward: -135.00. Mean loss target:  16.43. Mean loss:  34.13.\n",
      "Episode:  940. Mean training reward: -449.41. Validation reward: -249.84. Mean loss target:  16.44. Mean loss:  33.80.\n",
      "Episode:  950. Mean training reward: -528.66. Validation reward: -1362.73. Mean loss target:  16.52. Mean loss:  33.45.\n",
      "Episode:  960. Mean training reward: -846.54. Validation reward: -118.71. Mean loss target:  16.65. Mean loss:  33.00.\n",
      "Episode:  970. Mean training reward: -567.79. Validation reward: -621.32. Mean loss target:  16.74. Mean loss:  32.57.\n",
      "Episode:  980. Mean training reward: -282.27. Validation reward: -1219.20. Mean loss target:  16.78. Mean loss:  32.19.\n",
      "Episode:  990. Mean training reward: -297.99. Validation reward: -128.51. Mean loss target:  16.85. Mean loss:  31.77.\n",
      "Noise is changed\n",
      "0.025\n",
      "Episode: 1000. Mean training reward: -307.19. Validation reward: -267.70. Mean loss target:  16.92. Mean loss:  31.35.\n",
      "Episode: 1010. Mean training reward: -318.47. Validation reward: -242.90. Mean loss target:  16.98. Mean loss:  30.93.\n",
      "Episode: 1020. Mean training reward: -381.78. Validation reward: -880.91. Mean loss target:  17.05. Mean loss:  30.49.\n",
      "Episode: 1030. Mean training reward: -189.15. Validation reward: -263.17. Mean loss target:  17.08. Mean loss:  30.12.\n",
      "Episode: 1040. Mean training reward: -273.66. Validation reward: -235.34. Mean loss target:  17.08. Mean loss:  29.77.\n",
      "Episode: 1050. Mean training reward: -211.00. Validation reward: -126.76. Mean loss target:  17.11. Mean loss:  29.38.\n",
      "Episode: 1060. Mean training reward: -373.00. Validation reward: -888.77. Mean loss target:  17.11. Mean loss:  29.04.\n",
      "Episode: 1070. Mean training reward: -577.00. Validation reward: -125.15. Mean loss target:  17.26. Mean loss:  28.57.\n",
      "Episode: 1080. Mean training reward: -337.59. Validation reward: -343.61. Mean loss target:  17.38. Mean loss:  28.12.\n",
      "Episode: 1090. Mean training reward: -567.35. Validation reward: -814.57. Mean loss target:  17.51. Mean loss:  27.66.\n",
      "Episode: 1100. Mean training reward: -366.27. Validation reward: -246.48. Mean loss target:  17.61. Mean loss:  27.24.\n",
      "Episode: 1110. Mean training reward: -397.76. Validation reward: -128.19. Mean loss target:  17.69. Mean loss:  26.86.\n",
      "Episode: 1120. Mean training reward: -480.90. Validation reward: -1129.86. Mean loss target:  17.76. Mean loss:  26.43.\n",
      "Episode: 1130. Mean training reward: -437.85. Validation reward: -246.69. Mean loss target:  17.89. Mean loss:  25.97.\n",
      "Episode: 1140. Mean training reward: -658.89. Validation reward: -1015.94. Mean loss target:  17.98. Mean loss:  25.51.\n",
      "Episode: 1150. Mean training reward: -842.65. Validation reward: -1020.91. Mean loss target:  18.10. Mean loss:  25.06.\n",
      "Episode: 1160. Mean training reward: -902.31. Validation reward: -1469.46. Mean loss target:  18.23. Mean loss:  24.60.\n",
      "Episode: 1170. Mean training reward: -802.57. Validation reward: -1076.52. Mean loss target:  18.34. Mean loss:  24.23.\n",
      "Episode: 1180. Mean training reward: -723.21. Validation reward: -243.99. Mean loss target:  18.39. Mean loss:  23.93.\n",
      "Episode: 1190. Mean training reward: -420.76. Validation reward: -492.53. Mean loss target:  18.38. Mean loss:  23.69.\n",
      "Episode: 1200. Mean training reward: -430.16. Validation reward: -248.66. Mean loss target:  18.37. Mean loss:  23.48.\n",
      "Episode: 1210. Mean training reward: -267.55. Validation reward: -1099.27. Mean loss target:  18.34. Mean loss:  23.29.\n",
      "Episode: 1220. Mean training reward: -267.52. Validation reward: -121.29. Mean loss target:  18.32. Mean loss:  23.10.\n",
      "Episode: 1230. Mean training reward: -348.08. Validation reward: -273.13. Mean loss target:  18.29. Mean loss:  22.92.\n",
      "Episode: 1240. Mean training reward: -570.75. Validation reward: -125.98. Mean loss target:  18.31. Mean loss:  22.70.\n",
      "Episode: 1250. Mean training reward: -586.46. Validation reward: -1059.32. Mean loss target:  18.32. Mean loss:  22.49.\n",
      "Episode: 1260. Mean training reward: -253.24. Validation reward: -121.20. Mean loss target:  18.32. Mean loss:  22.29.\n",
      "Episode: 1270. Mean training reward: -242.87. Validation reward:  -0.59. Mean loss target:  18.26. Mean loss:  22.16.\n",
      "Episode: 1280. Mean training reward: -275.66. Validation reward: -525.46. Mean loss target:  18.19. Mean loss:  22.05.\n",
      "Episode: 1290. Mean training reward: -467.77. Validation reward: -119.23. Mean loss target:  18.20. Mean loss:  21.86.\n",
      "Episode: 1300. Mean training reward: -260.53. Validation reward: -121.67. Mean loss target:  18.20. Mean loss:  21.66.\n",
      "Episode: 1310. Mean training reward: -362.13. Validation reward: -228.84. Mean loss target:  18.21. Mean loss:  21.48.\n",
      "Episode: 1320. Mean training reward: -228.13. Validation reward: -121.66. Mean loss target:  18.19. Mean loss:  21.32.\n",
      "Episode: 1330. Mean training reward: -247.16. Validation reward: -513.33. Mean loss target:  18.20. Mean loss:  21.16.\n",
      "Episode: 1340. Mean training reward: -251.36. Validation reward: -124.21. Mean loss target:  18.19. Mean loss:  21.01.\n",
      "Episode: 1350. Mean training reward: -154.23. Validation reward: -250.42. Mean loss target:  18.13. Mean loss:  20.92.\n",
      "Episode: 1360. Mean training reward: -186.68. Validation reward: -491.03. Mean loss target:  18.07. Mean loss:  20.82.\n",
      "Episode: 1370. Mean training reward: -369.49. Validation reward: -120.88. Mean loss target:  18.05. Mean loss:  20.70.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1380. Mean training reward: -238.47. Validation reward: -255.30. Mean loss target:  17.99. Mean loss:  20.61.\n",
      "Episode: 1390. Mean training reward: -241.71. Validation reward: -128.07. Mean loss target:  17.93. Mean loss:  20.52.\n",
      "Episode: 1400. Mean training reward: -374.94. Validation reward: -635.27. Mean loss target:  17.89. Mean loss:  20.41.\n",
      "Episode: 1410. Mean training reward: -346.96. Validation reward: -244.33. Mean loss target:  17.85. Mean loss:  20.30.\n",
      "Episode: 1420. Mean training reward: -254.35. Validation reward: -859.80. Mean loss target:  17.80. Mean loss:  20.22.\n",
      "Episode: 1430. Mean training reward: -447.93. Validation reward: -392.56. Mean loss target:  17.77. Mean loss:  20.11.\n",
      "Episode: 1440. Mean training reward: -292.49. Validation reward: -119.85. Mean loss target:  17.72. Mean loss:  20.05.\n",
      "Episode: 1450. Mean training reward: -229.26. Validation reward: -245.59. Mean loss target:  17.67. Mean loss:  19.98.\n",
      "Episode: 1460. Mean training reward: -275.97. Validation reward: -125.65. Mean loss target:  17.60. Mean loss:  19.90.\n",
      "Episode: 1470. Mean training reward: -316.65. Validation reward: -128.68. Mean loss target:  17.55. Mean loss:  19.82.\n",
      "Episode: 1480. Mean training reward: -171.02. Validation reward: -259.13. Mean loss target:  17.50. Mean loss:  19.73.\n",
      "Episode: 1490. Mean training reward: -313.30. Validation reward: -883.04. Mean loss target:  17.44. Mean loss:  19.63.\n",
      "Episode: 1500. Mean training reward: -260.07. Validation reward: -501.46. Mean loss target:  17.40. Mean loss:  19.53.\n",
      "Episode: 1510. Mean training reward: -275.64. Validation reward: -750.87. Mean loss target:  17.36. Mean loss:  19.42.\n",
      "Episode: 1520. Mean training reward: -198.58. Validation reward: -251.22. Mean loss target:  17.32. Mean loss:  19.31.\n",
      "Episode: 1530. Mean training reward: -331.54. Validation reward: -122.91. Mean loss target:  17.29. Mean loss:  19.18.\n",
      "Episode: 1540. Mean training reward: -344.11. Validation reward: -881.55. Mean loss target:  17.25. Mean loss:  19.05.\n",
      "Episode: 1550. Mean training reward: -266.22. Validation reward: -126.10. Mean loss target:  17.22. Mean loss:  18.95.\n",
      "Episode: 1560. Mean training reward: -206.29. Validation reward: -121.92. Mean loss target:  17.18. Mean loss:  18.83.\n",
      "Episode: 1570. Mean training reward: -293.45. Validation reward: -121.79. Mean loss target:  17.15. Mean loss:  18.71.\n",
      "Episode: 1580. Mean training reward: -214.95. Validation reward: -122.48. Mean loss target:  17.10. Mean loss:  18.60.\n",
      "Episode: 1590. Mean training reward: -233.45. Validation reward: -247.15. Mean loss target:  17.06. Mean loss:  18.49.\n",
      "Episode: 1600. Mean training reward: -326.42. Validation reward: -255.01. Mean loss target:  17.01. Mean loss:  18.38.\n",
      "Episode: 1610. Mean training reward: -191.92. Validation reward: -377.27. Mean loss target:  16.96. Mean loss:  18.28.\n",
      "Episode: 1620. Mean training reward: -175.37. Validation reward: -129.98. Mean loss target:  16.89. Mean loss:  18.24.\n",
      "Episode: 1630. Mean training reward: -198.94. Validation reward: -243.21. Mean loss target:  16.82. Mean loss:  18.19.\n",
      "Episode: 1640. Mean training reward: -192.11. Validation reward: -127.87. Mean loss target:  16.74. Mean loss:  18.13.\n",
      "Episode: 1650. Mean training reward: -130.39. Validation reward: -130.13. Mean loss target:  16.67. Mean loss:  18.08.\n",
      "Episode: 1660. Mean training reward: -159.74. Validation reward: -762.56. Mean loss target:  16.59. Mean loss:  18.03.\n",
      "Episode: 1670. Mean training reward: -693.97. Validation reward: -494.10. Mean loss target:  16.59. Mean loss:  17.85.\n",
      "Episode: 1680. Mean training reward: -632.33. Validation reward: -1164.94. Mean loss target:  16.57. Mean loss:  17.77.\n",
      "Episode: 1690. Mean training reward: -487.03. Validation reward: -1019.42. Mean loss target:  16.55. Mean loss:  17.60.\n",
      "Episode: 1700. Mean training reward: -338.17. Validation reward: -127.32. Mean loss target:  16.49. Mean loss:  17.48.\n",
      "Episode: 1710. Mean training reward: -151.45. Validation reward: -131.50. Mean loss target:  16.42. Mean loss:  17.42.\n",
      "Episode: 1720. Mean training reward: -155.16. Validation reward: -116.89. Mean loss target:  16.35. Mean loss:  17.36.\n"
     ]
    }
   ],
   "source": [
    "# Continuous mountain car problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# training settings\n",
    "noise_size = 0.1\n",
    "learning_rate = 0.001# you know this by now\n",
    "episodes = 10000 #Number of episodes (Outer loop)\n",
    "maxTime = 200 #Maximum time for an episode (Inner loop)\n",
    "epsilon = 0.1 #The epsilon in the epsilon-greedy method for exploration\n",
    "gamma = 0.95 #Discount factor\n",
    "valid_episode = 10 #Compute validation at every valid_episode'th episode\n",
    "ep = 0 #Episode counter for valid_episode\n",
    "Nd = 100000 #Size of D matrix for experience replay\n",
    "Nb = 64 #Size of minibatch in the stochastic gradient descent with experience replay\n",
    "ex_rep_counter = 1\n",
    "try:\n",
    "    statistics = []\n",
    "    r_stat = np.zeros((maxTime,valid_episode))\n",
    "    loss_stat = []\n",
    "    loss2_stat = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(copy_target_actor)\n",
    "        sess.run(copy_target_critic)\n",
    "        sess.run(copy_criact_critic)\n",
    "        print('start training')\n",
    "        D = np.zeros((Nd,n_states*2+1+1+1)) #5 is for (aj,xj,rj,xj+1,done). It's the history matrix for experience replay\n",
    "        D_counter = 0 #Counter in circular D-buffer\n",
    "        for episode in range(episodes):\n",
    "            x = env.reset()\n",
    "            a_noise = 0\n",
    "            for timestep in range(maxTime):\n",
    "                # Action noise strategy (at least respect the bounds on the action)\n",
    "                #------------------------------------------------------------------\n",
    "                at = sess.run(fetches=l_out_act, feed_dict={states_pl: np.reshape(x,(1,n_states))}) #Take action according to current best policy\n",
    "                #print(at)\n",
    "                a_noise = a_noise*0.9 + noise_size*np.random.normal(0,1,1) #sample the exploration noise from a normal distribution (can be made more sophisticated)\n",
    "                at = at + a_noise\n",
    "                at = np.clip(at,-2,2)\n",
    "                #Apply action and observe reward and the new state\n",
    "                #-------------------------------------------------\n",
    "                \n",
    "                #print(at)\n",
    "                #print(x)\n",
    "                x_next, r, done, _ = env.step(at) #Apply the action\n",
    "\n",
    "                r_stat[timestep,ep] = r #Save for statistics\n",
    "                \n",
    "                #Save step into D-buffer for Experience replay\n",
    "                #---------------------------------------------\n",
    "                #Update into a a circular buffer\n",
    "                update_index_ER = D_counter\n",
    "                D[update_index_ER,:] = np.concatenate((np.reshape(x,n_states),np.reshape(at,n_actions),np.reshape(r,1),np.reshape(x_next,n_states),np.reshape(done,1)))\n",
    "                D_counter = D_counter + 1\n",
    "                if D_counter == Nd:\n",
    "                    D_counter = 0\n",
    "\n",
    "                #Sample the minibatch with Experience replay\n",
    "                #------------------------------------------------------------\n",
    "                y_mb = np.zeros((Nb,1))\n",
    "                at_mb = np.zeros((Nb,n_actions))\n",
    "                x_mb = np.zeros((Nb,n_states))\n",
    "                for batch_number in range(Nb):\n",
    "                    sample_index_ER = np.random.randint(0,ex_rep_counter)\n",
    "                    Drow= D[sample_index_ER,:]\n",
    "                    #Unpack the history quintuple\n",
    "                    xj = Drow[0:n_states]\n",
    "                    aj = Drow[n_states]\n",
    "                    rj = Drow[n_states+1]\n",
    "                    xj1 = Drow[n_states+2:2*n_states+2]\n",
    "                    done1 = Drow[-1]\n",
    "                    \n",
    "                    #Fill out the y-value (right hand side of the Bellman equation)\n",
    "                    if done1:  #If we have terminated the game\n",
    "                        y_mb[batch_number] = rj\n",
    "                    else: #If we have not terminated the game \n",
    "                        a_next = sess.run(fetches=l_out_act_t, feed_dict={states_pl: np.reshape(xj1,(1,n_states))})\n",
    "                        value_next = sess.run(fetches=l_out_cri_t, feed_dict={sta_act_pl: np.column_stack((np.reshape(xj1,(1,n_states)),np.reshape(a_next,(1,1))))})\n",
    "                        y_mb[batch_number,:] = rj+gamma*value_next\n",
    "                        \n",
    "                    #Fill out the action a_mb\n",
    "                    at_mb[batch_number] = aj\n",
    "                    \n",
    "                    #Fill out the state x_mb\n",
    "                    x_mb[batch_number,:] = xj\n",
    "                \n",
    "                \n",
    "                if ex_rep_counter != Nd: # Handle initial filling of experience replay\n",
    "                    ex_rep_counter = ex_rep_counter + 1\n",
    "                #Do a stochastic gradient descent on the critic network\n",
    "                #------------------------------------------------------\n",
    "                loss,_ = sess.run(fetches=[loss_f,train_f], feed_dict={\n",
    "                    targets_pl: y_mb,\n",
    "                    sta_act_pl: np.column_stack((x_mb,at_mb)),\n",
    "                    learning_rate_pl: learning_rate\n",
    "                })    \n",
    "                \n",
    "                loss_stat.append(loss) #Save for statistics\n",
    "                sess.run(copy_criact_critic)\n",
    "                # Compute the gradients for the actor network (with same mini-batches)\n",
    "                #---------------------------------------------------------------------\n",
    "                \n",
    "                #print(pa_h[0].eval())\n",
    "                loss2,_,gc = sess.run(fetches=[loss_c,train_c,g_c], feed_dict={\n",
    "                    states_pl: x_mb,\n",
    "                    learning_rate_pl: learning_rate\n",
    "                })  \n",
    "                loss2_stat.append(loss2) #Save for statistics\n",
    "                #print('-')\n",
    "                #print(gc)\n",
    "                #print(pa_h[0].eval())\n",
    "                #print(sess.run(cl_out_cri,feed_dict={states_pl: x_mb}))\n",
    "                #print('-')\n",
    "\n",
    "\n",
    "                \n",
    "                # Update target networks\n",
    "                #------------------------------\n",
    "                ops = sess.run(update_target_actor)  #Update the actor target network\n",
    "                ops = sess.run(update_target_critic) #Update the critic target network\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                #Prepare for next timestep\n",
    "                #----------------------------\n",
    "                \n",
    "                x = x_next\n",
    "                if done: \n",
    "                    #print('Done before time!!!')\n",
    "                    break #Game over! And end the episode\n",
    "                \n",
    "             #Validation time!\n",
    "            #----------------\n",
    "            if (episode % 1000) == 0:\n",
    "                noise_size = noise_size/2\n",
    "                print('Noise is changed')\n",
    "                print(noise_size)\n",
    "            \n",
    "            if (episode % valid_episode) == 0: \n",
    "                #Computation of validation by playing the game once\n",
    "                #--------------------------------------------------\n",
    "                x = env.reset()\n",
    "                r_valid = np.zeros(maxTime)\n",
    "                #print(x)\n",
    "                for time_valid in range(maxTime):\n",
    "                    #Only pick greedy action in validation!\n",
    "                    action = sess.run(fetches=l_out_act, feed_dict={states_pl: [x]})\n",
    "                    at = action\n",
    "                \n",
    "                    x_next, r, done, _ = env.step(at) #Apply the action\n",
    "                    r_valid[time_valid] = r #Save for statistics\n",
    "                    if done: break #Game over! And end the episode\n",
    "                    x = np.reshape(x_next,(n_states,))\n",
    "                \n",
    "                \n",
    "                #Print out validation statistics\n",
    "                #-------------------------------\n",
    "                r_stat_mean = np.mean(np.sum(r_stat,axis=0))\n",
    "                r_valid_sum = np.sum(r_valid)\n",
    "                loss_stat_mean = np.mean(loss_stat,axis=0)\n",
    "                loss2_stat_mean = np.mean(loss2_stat,axis=0)\n",
    "                print('Episode: %4d. Mean training reward: %6.2f. Validation reward: %6.2f. Mean loss target: %6.2f. Mean loss: %6.2f.' % (episode, r_stat_mean, r_valid_sum, loss_stat_mean, loss2_stat_mean))\n",
    "                #Reset statistics variables\n",
    "                #--------------------------\n",
    "                ep = 0 #Reset episode counter between validations\n",
    "                r_stat = np.zeros((maxTime,valid_episode)) \n",
    "                #saver.save(sess, 'tmp2/model.ckpt')\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                ep = ep + 1\n",
    "                \n",
    "        print('done')\n",
    "        # save session\n",
    "        saver.save(sess, 'tmp2/model.ckpt')\n",
    "except KeyboardInterrupt:\n",
    "    pass            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-27 14:14:33,538] Restoring parameters from tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.42629543 -0.904584    0.09391583]\n",
      "[-119.01763153]\n"
     ]
    }
   ],
   "source": [
    "# review solution\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"tmp2/model.ckpt\")\n",
    "    x = env.reset()\n",
    "    print(x)\n",
    "    view = Viewer(env, custom_render=True)\n",
    "    r_sum = 0\n",
    "    for _ in range(100):\n",
    "        env.render() # uncomment this to use gym's own render function\n",
    "        #a = get_action(sess, s, stochastic=False)\n",
    "        a = sess.run(fetches=l_out_act, feed_dict={states_pl: np.reshape(x,(1,n_states))})\n",
    "        #print(a)\n",
    "        #value = sess.run(fetches=l_out, feed_dict={states_pl: [x],is_training_pl: False})\n",
    "        #a=value.argmax()\n",
    "        x, r, done, _ = env.step(a)\n",
    "        r_sum = r_sum + r\n",
    "    env.render(close=True) # uncomment this to use gym'm own render function\n",
    "    print(r_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp2/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-27 14:13:15,920] Restoring parameters from tmp2/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.63175869]]\n",
      "[[-48.95029831]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"tmp2/model.ckpt\")\n",
    "    x = [0,0,0]\n",
    "    a = sess.run(fetches=l_out_act, feed_dict={states_pl: np.reshape(x,(1,n_states))})\n",
    "    b = sess.run(fetches=l_out_cri, feed_dict={sta_act_pl: np.column_stack((np.reshape(x,(1,n_states)),np.reshape(a,(1,1))))})\n",
    "    print(a)\n",
    "    print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
