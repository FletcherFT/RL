{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient Robert Miklos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.nn import relu, softmax\n",
    "from tensorflow.contrib.layers import fully_connected, convolution2d, flatten, batch_norm, max_pool2d, dropout\n",
    "import gym\n",
    "from gym import envs\n",
    "from utils import Viewer\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-03 22:02:06,820] Making new env: Pendulum-v0\n"
     ]
    }
   ],
   "source": [
    "# create gym environment\n",
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demo the environment\n",
    "env.reset() # reset the environment\n",
    "#view = Viewer(env, custom_render=True) # we use this custom viewer to render the environment inline in the notebook\n",
    "for timestep in range(200):\n",
    "#   #view.render()\n",
    "    env.render() # uncomment this to use gym's own render function\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action) # take a random action\n",
    "##view.render(close=True, display_gif=True) # display the environment inline in the notebook\n",
    "env.render(close=True) # uncomment this to use gym'm own render function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1)\n",
      "(?, 1)\n",
      "(?, 1)\n",
      "(?, 1)\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "n_actions = 1 # Power to the engine\n",
    "n_states = 3 # Position and velocity\n",
    "n_values = 1 # Number of value functions\n",
    "\n",
    "# Tuning the neural networks\n",
    "n_hidden_act = 100 # Number of neurons in the hidden layer by the actor\n",
    "n_hidden_cri = 100 # Number of neurons in the hidden layer by the critic\n",
    "n_hidden_act_t = n_hidden_act# Number of neurons in the hidden layer by the target actor\n",
    "n_hidden_cri_t = n_hidden_cri# Number of neurons in the hidden layer by the target critic\n",
    "\n",
    "\n",
    "# Defining the four neural networks: actor, critic, target actor, target critic.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "states_pl = tf.placeholder(tf.float32, [None, n_states], name='states_pl') # States of the environment\n",
    "actions_pl = tf.placeholder(tf.float32, [None, n_actions], name='actions_pl') # Action to the environment\n",
    "values_pl = tf.placeholder(tf.float32, [None, n_values], name='values_pl') # The action values\n",
    "sta_act_pl = tf.placeholder(tf.float32, [None, n_states+n_actions], name='sta_act_pl') # The concatenation of states and actions (for critic network)\n",
    "targets_pl = tf.placeholder(tf.float32, [None, n_values], name='targets_pl') # The target for the critic network\n",
    "learning_rate_pl = tf.placeholder(tf.float32, name='learning_rate_pl') # Learning rate\n",
    "\n",
    "scale = 2\n",
    "# The actor network\n",
    "l_hidden_act = tf.layers.dense(inputs=states_pl, units=n_hidden_act, activation=relu, name='l_hidden_act')\n",
    "l_out_act = tf.scalar_mul(scale,tf.layers.dense(inputs=l_hidden_act, units=n_actions, activation=tf.tanh, name='l_out_act') )\n",
    "\n",
    "# The target actor network\n",
    "l_hidden_act_t = tf.layers.dense(inputs=states_pl, units=n_hidden_act_t, activation=relu, name='tl_hidden_act')\n",
    "l_out_act_t = tf.scalar_mul(scale,tf.layers.dense(inputs=l_hidden_act_t, units=n_actions, activation=tf.tanh, name='tl_out_act') )\n",
    "\n",
    "# The critic network\n",
    "l_hidden_cri = tf.layers.dense(inputs=sta_act_pl, units=n_hidden_cri, activation=relu, name='l_hidden_cri',bias_initializer=tf.random_uniform_initializer)\n",
    "l_out_cri = tf.layers.dense(inputs=l_hidden_cri, units=n_values, activation=None, name='l_out_cri',bias_initializer=tf.random_uniform_initializer) \n",
    "\n",
    "# The target critic network\n",
    "l_hidden_cri_t = tf.layers.dense(inputs=sta_act_pl, units=n_hidden_cri_t, activation=relu, name='tl_hidden_cri')\n",
    "l_out_cri_t = tf.layers.dense(inputs=l_hidden_cri_t, units=n_values, activation=None, name='tl_out_cri') \n",
    "\n",
    "# The critic network for searching for the parameters of the actor network\n",
    "cl_hidden_cri = tf.layers.dense(inputs=tf.concat([states_pl,l_out_act],axis=1), units=n_hidden_cri, activation=relu, name='cl_hidden_cri',bias_initializer=tf.random_uniform_initializer)\n",
    "cl_out_cri = tf.layers.dense(inputs=cl_hidden_cri, units=n_values, activation=None, name='cl_out_cri',bias_initializer=tf.random_uniform_initializer) \n",
    "\n",
    "\n",
    "\n",
    "print(l_out_act.get_shape())\n",
    "print(l_out_act_t.get_shape())\n",
    "print(l_out_cri.get_shape())\n",
    "print(l_out_cri_t.get_shape())\n",
    "print(cl_out_cri.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.26726086 -0.96362422 -0.41580972]\n",
      "[[-0.38118222]]\n",
      "-\n",
      "[-0.26726086 -0.96362422 -0.41580972]\n",
      "[[-0.23895276]]\n",
      "-\n",
      "[-0.26726086 -0.96362422 -0.41580972]\n",
      "[[-0.2681706]]\n",
      "-------------------------------------------------------------------------\n",
      "[-0.75424304 -0.65659534  0.64760827]\n",
      "[[-0.38648438]]\n",
      "-\n",
      "[-0.75424304 -0.65659534  0.64760827]\n",
      "[[-0.08774559]]\n",
      "-\n",
      "[[ 0.11491752]\n",
      " [-0.07824191]\n",
      " [-0.02269061]\n",
      " [-0.03692862]\n",
      " [-0.23683563]\n",
      " [-0.24235514]\n",
      " [-0.13943797]\n",
      " [-0.16787541]\n",
      " [-0.13283487]\n",
      " [-0.03284979]\n",
      " [ 0.20712107]\n",
      " [ 0.12216297]\n",
      " [-0.23600475]\n",
      " [-0.09536099]\n",
      " [-0.02147691]\n",
      " [ 0.10448894]\n",
      " [ 0.09327894]\n",
      " [ 0.04959184]\n",
      " [-0.18513831]\n",
      " [ 0.06669495]\n",
      " [-0.01242921]\n",
      " [-0.02705762]\n",
      " [-0.18773621]\n",
      " [ 0.14716128]\n",
      " [-0.07080835]\n",
      " [ 0.07020062]\n",
      " [ 0.01253533]\n",
      " [ 0.13690585]\n",
      " [ 0.07723156]\n",
      " [-0.01841506]\n",
      " [ 0.16174939]\n",
      " [ 0.22334003]\n",
      " [ 0.07610381]\n",
      " [-0.14489746]\n",
      " [-0.01533392]\n",
      " [ 0.04096672]\n",
      " [-0.03758323]\n",
      " [ 0.07834896]\n",
      " [ 0.21396616]\n",
      " [-0.19937791]\n",
      " [-0.05366522]\n",
      " [ 0.19504321]\n",
      " [-0.16363543]\n",
      " [ 0.11572799]\n",
      " [-0.06306809]\n",
      " [ 0.23600483]\n",
      " [ 0.1824832 ]\n",
      " [ 0.14367232]\n",
      " [ 0.14430007]\n",
      " [ 0.08249071]\n",
      " [ 0.00188267]\n",
      " [ 0.1603699 ]\n",
      " [ 0.10732538]\n",
      " [ 0.04676756]\n",
      " [-0.13483858]\n",
      " [-0.23041667]\n",
      " [-0.08807817]\n",
      " [-0.1859718 ]\n",
      " [-0.1052316 ]\n",
      " [-0.15142357]\n",
      " [ 0.13925168]\n",
      " [ 0.14346167]\n",
      " [-0.14959559]\n",
      " [-0.02274105]\n",
      " [-0.22144598]\n",
      " [ 0.11125401]\n",
      " [ 0.0530017 ]\n",
      " [-0.14887698]\n",
      " [ 0.10785255]\n",
      " [ 0.22263062]\n",
      " [ 0.23777696]\n",
      " [ 0.05243489]\n",
      " [-0.04747161]\n",
      " [-0.00375853]\n",
      " [-0.18012229]\n",
      " [-0.06685841]\n",
      " [ 0.08325651]\n",
      " [-0.10001129]\n",
      " [-0.24159923]\n",
      " [-0.05786365]\n",
      " [ 0.08632848]\n",
      " [-0.02556719]\n",
      " [-0.16658589]\n",
      " [ 0.10940725]\n",
      " [-0.14646511]\n",
      " [-0.10204342]\n",
      " [-0.02118072]\n",
      " [-0.16032884]\n",
      " [-0.04851761]\n",
      " [ 0.04914457]\n",
      " [-0.102744  ]\n",
      " [ 0.08290437]\n",
      " [-0.0203789 ]\n",
      " [-0.12134559]\n",
      " [-0.22270505]\n",
      " [ 0.12487993]\n",
      " [ 0.06092468]\n",
      " [-0.12108567]\n",
      " [ 0.21829221]\n",
      " [ 0.21888486]]\n",
      "[[ -4.60764021e-02   2.51978636e-05  -1.34809434e-01  -5.98141253e-02\n",
      "    1.15435570e-01  -1.38896704e-01  -3.82722020e-02  -4.60897386e-02\n",
      "   -1.34543777e-01   2.17708051e-01  -2.25840613e-01  -1.89231932e-01\n",
      "   -7.96646774e-02  -1.39441192e-01   1.87013596e-01  -1.65728241e-01\n",
      "   -9.40952748e-02   2.01131701e-01  -1.14519700e-01   1.81586385e-01\n",
      "   -6.43231571e-02   5.54565191e-02  -1.12301767e-01  -1.03775516e-01\n",
      "   -2.82734036e-02   9.69840288e-02   1.52051330e-01   3.35756242e-02\n",
      "   -5.10931015e-04  -3.93460095e-02   1.80556446e-01  -7.55333155e-02\n",
      "    1.00772917e-01  -2.30918929e-01  -1.45042971e-01   1.53588712e-01\n",
      "    1.62776917e-01   1.78676099e-01  -2.19815686e-01   9.54737961e-02\n",
      "   -2.74860412e-02   1.21727794e-01  -1.45297691e-01   2.12533355e-01\n",
      "   -1.42322078e-01  -2.15703219e-01  -1.57880366e-01   1.53716415e-01\n",
      "    1.67241693e-01  -1.20473444e-01  -6.02467209e-02  -8.33307505e-02\n",
      "   -4.76640463e-02   2.39629030e-01   7.81083107e-03  -2.35458836e-01\n",
      "    1.66340649e-01   1.03236288e-01   2.37172574e-01  -5.92576116e-02\n",
      "   -1.98483139e-01   5.57283461e-02   8.25992227e-03   2.77833641e-02\n",
      "   -5.91509789e-02  -6.50338978e-02  -6.14127070e-02  -1.97562695e-01\n",
      "   -1.39455378e-01   1.95813268e-01  -2.12408066e-01   6.30351305e-02\n",
      "    3.46162915e-03  -3.77905965e-02  -5.87448478e-03  -7.59014338e-02\n",
      "    2.38854617e-01  -1.88491315e-01  -2.27940798e-01   1.98481470e-01\n",
      "   -1.17893659e-01   1.70865446e-01  -5.91323078e-02  -1.68658853e-01\n",
      "   -5.60399890e-02   4.94198501e-02  -2.04923302e-01   1.43283248e-01\n",
      "    1.51638001e-01   1.96674079e-01   1.88490808e-01   1.26268864e-01\n",
      "    4.74604666e-02   1.47513211e-01  -7.41374493e-02   9.94325578e-02\n",
      "    2.36198723e-01  -1.75423145e-01  -2.06282169e-01   9.23297107e-02]\n",
      " [  6.52949810e-02   6.24015927e-02   9.65438187e-02   1.95877403e-01\n",
      "   -2.47254670e-02   1.62958860e-01  -1.19867742e-01  -7.26482421e-02\n",
      "    4.66247797e-02   1.80136442e-01   2.28894740e-01   1.04386359e-01\n",
      "    1.67399853e-01   7.42705464e-02  -1.93530917e-02  -1.30198747e-01\n",
      "   -2.36254945e-01  -3.34552526e-02  -4.94478941e-02  -2.04491675e-01\n",
      "   -1.29288048e-01   1.38014108e-01  -1.92479223e-01  -2.05196694e-01\n",
      "    1.62788242e-01  -1.06167927e-01   1.45531923e-01  -2.12966517e-01\n",
      "   -9.51197147e-02  -7.62247592e-02  -1.58206433e-01   1.44759685e-01\n",
      "    1.06733888e-01  -9.78939235e-02   2.00350702e-01  -1.22421645e-01\n",
      "    5.14044464e-02  -8.93746465e-02   2.29680449e-01  -1.81022063e-01\n",
      "    1.60992146e-03  -2.01978892e-01   1.30339801e-01  -2.30732188e-01\n",
      "    2.19975471e-01   6.24546409e-03   1.87115192e-01  -1.98131412e-01\n",
      "   -1.38408095e-02  -3.17715108e-02   1.90760881e-01  -2.38404214e-01\n",
      "   -1.90135837e-03  -1.13269746e-01  -2.32114598e-01   1.30590439e-01\n",
      "   -1.54684037e-01   2.01921225e-01   1.59825474e-01   1.89731747e-01\n",
      "    1.60524845e-01   1.83160603e-03   1.48314536e-01   3.25937271e-02\n",
      "   -5.83983958e-02   2.29463160e-01   1.95313781e-01   1.93310976e-02\n",
      "   -1.38974190e-03  -2.01372445e-01   1.58150643e-01  -1.47005379e-01\n",
      "   -4.61177975e-02   2.28129387e-01   2.05427527e-01   2.26560175e-01\n",
      "    3.42079699e-02  -8.48344564e-02  -2.27601498e-01   3.96237373e-02\n",
      "   -3.09132040e-02   4.10644412e-02   1.51832223e-01   4.14956212e-02\n",
      "    1.66561514e-01   4.71794009e-02  -1.49247587e-01   1.39142990e-01\n",
      "    7.61168599e-02   2.62521207e-02  -7.05679953e-02   1.77999943e-01\n",
      "   -6.32299483e-03  -1.73734874e-01  -1.09894812e-01   1.55407190e-01\n",
      "    1.17163450e-01   1.70417666e-01  -2.54190713e-02  -2.27037027e-01]\n",
      " [  5.41993380e-02  -2.01303035e-01   1.21823668e-01   2.26968884e-01\n",
      "   -4.49296981e-02  -2.16973096e-01  -3.79402339e-02   2.12264895e-01\n",
      "    5.48832715e-02   1.90877587e-01   1.39067739e-01  -1.54017627e-01\n",
      "    1.40633404e-01  -1.94989324e-01   1.25812620e-01   3.33822370e-02\n",
      "   -2.10090950e-01   9.40922499e-02  -7.34705925e-02   9.11218822e-02\n",
      "   -3.71776819e-02   2.27786630e-01  -2.24594846e-01   1.90379202e-01\n",
      "   -8.49091858e-02   1.55455768e-01  -1.17883407e-01   2.38652527e-01\n",
      "   -1.49733946e-01   2.26741523e-01   1.20696545e-01  -4.88622338e-02\n",
      "    6.18721545e-03  -5.98073751e-02  -3.21439058e-02   3.90161574e-02\n",
      "   -5.82536161e-02  -2.20478773e-01  -5.40203303e-02  -2.37417459e-01\n",
      "   -1.28323555e-01  -9.93472189e-02  -3.32583189e-02  -1.20941080e-01\n",
      "   -1.98800683e-01   1.86029017e-01   5.32204807e-02  -6.76771253e-02\n",
      "    1.20793045e-01   2.19177961e-01   1.58598721e-01   1.97972029e-01\n",
      "   -9.81715471e-02  -5.09566367e-02  -1.66915894e-01  -9.64320749e-02\n",
      "   -2.28163391e-01   9.50875282e-02  -4.21313792e-02  -4.61196303e-02\n",
      "    2.26791739e-01  -4.30358499e-02   3.53454947e-02  -6.52272254e-02\n",
      "   -1.56655833e-01   1.16258979e-01  -2.28930414e-01  -9.99463499e-02\n",
      "   -8.93042684e-02   3.25397253e-02   6.70820177e-02   1.72727048e-01\n",
      "    1.84068680e-02  -2.03269854e-01   1.88736707e-01  -4.09498066e-02\n",
      "   -1.42265439e-01   1.56928033e-01  -2.17861757e-01   1.90432936e-01\n",
      "   -1.75702155e-01   5.03189266e-02  -9.57149267e-03   1.52348787e-01\n",
      "    2.38110662e-01   9.20183361e-02  -3.10197771e-02   3.57357264e-02\n",
      "    9.19575691e-02   2.10062653e-01  -1.27301708e-01   9.59734917e-02\n",
      "   -1.94190592e-02  -1.61351383e-01   1.08141661e-01  -7.96014667e-02\n",
      "   -1.74782276e-01   3.17991078e-02   8.87902081e-03   1.70981675e-01]\n",
      " [  1.25015527e-01   1.14784956e-01   2.19732314e-01  -2.54844725e-02\n",
      "   -2.05794945e-01  -7.47690499e-02  -1.25383228e-01  -1.26839131e-02\n",
      "    3.98526490e-02   1.76156044e-01  -1.24110714e-01   1.06247753e-01\n",
      "    1.28608942e-03  -1.60620436e-01  -1.79377958e-01  -1.55537024e-01\n",
      "    8.19522440e-02  -1.12641186e-01   1.85648322e-01   1.56917423e-01\n",
      "   -9.73322541e-02   1.20060027e-01   1.49029106e-01   3.66484821e-02\n",
      "   -2.89880186e-02   2.15400755e-01  -2.06123546e-01  -6.39015585e-02\n",
      "    2.28137612e-01  -2.21484542e-01  -1.39823616e-01   9.77932513e-02\n",
      "   -7.77956247e-02   4.52055633e-02   4.83215749e-02  -1.19358450e-02\n",
      "    1.14266932e-01   1.48641527e-01  -1.57956123e-01   1.12491280e-01\n",
      "    3.83827388e-02   2.18516290e-02  -8.47902447e-02   1.69392884e-01\n",
      "   -1.47477075e-01   6.47336245e-04   1.80712759e-01   6.52792752e-02\n",
      "    1.76740676e-01  -2.00214744e-01   4.25449610e-02   1.00695789e-01\n",
      "   -2.10823223e-01   1.67760044e-01   1.40265197e-01  -2.19682604e-02\n",
      "    2.99271941e-02   1.17181420e-01   1.80288017e-01  -1.76666558e-01\n",
      "    1.58518255e-01  -1.39846742e-01   1.70665413e-01  -1.62867740e-01\n",
      "   -1.02147773e-01  -2.06002891e-01  -1.40319616e-02  -1.61420390e-01\n",
      "   -2.08210781e-01  -7.23372847e-02   1.10891700e-01  -1.32242471e-01\n",
      "   -1.90107018e-01  -1.56150058e-01   1.07939690e-01  -1.57843143e-01\n",
      "   -1.96508765e-02   1.95821404e-01  -1.83377892e-01  -1.28658742e-01\n",
      "   -1.15432866e-01  -1.28399670e-01  -1.60743624e-01  -6.28727823e-02\n",
      "   -2.28874415e-01  -1.65197223e-01  -2.32225284e-01  -2.10812390e-01\n",
      "   -8.44705254e-02  -2.01128781e-01   7.25287795e-02   1.12832874e-01\n",
      "   -2.10571766e-01   8.27303231e-02   2.26915151e-01   2.08579481e-01\n",
      "   -1.70506984e-02  -2.04144537e-01  -1.68420553e-01   8.81410241e-02]]\n",
      "110011\n",
      "[ 0.00277817]\n",
      "[ 0.17387033  0.0765866   0.39398384  0.2972548   0.00954664  0.97748649\n",
      "  0.72166038  0.51925695  0.92903185  0.01333618  0.04980493  0.84448731\n",
      "  0.00143027  0.55329525  0.51134098  0.09663725  0.1602354   0.0010767\n",
      "  0.51615334  0.33521211  0.93848443  0.81477463  0.29399824  0.85305965\n",
      "  0.17528093  0.0288825   0.10976601  0.66047752  0.32900321  0.381145\n",
      "  0.87775517  0.22223461  0.98706055  0.13946033  0.964656    0.6506114\n",
      "  0.68484116  0.68502617  0.8860569   0.95342553  0.77695668  0.08648419\n",
      "  0.72677052  0.03319764  0.53157067  0.52811885  0.31121302  0.79165089\n",
      "  0.60270369  0.06560755  0.22217596  0.07345641  0.6473707   0.46681297\n",
      "  0.47855926  0.55485797  0.8359139   0.10830963  0.82642615  0.39726603\n",
      "  0.73246777  0.36150956  0.83264959  0.87035131  0.31579614  0.74441421\n",
      "  0.5882163   0.53620148  0.82744455  0.38286424  0.92870796  0.79935944\n",
      "  0.59187591  0.88654733  0.49863243  0.33109426  0.48316133  0.28604019\n",
      "  0.58064544  0.12115026  0.71811748  0.26267278  0.16699171  0.78450966\n",
      "  0.66826928  0.69661164  0.05353522  0.59205616  0.26874733  0.07878947\n",
      "  0.10914671  0.75328231  0.0060339   0.23816848  0.74206722  0.12016213\n",
      "  0.45433152  0.16276777  0.13774312  0.94889903]\n",
      "110011\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# test forward pass of actor network\n",
    "state = env.reset()\n",
    "#yy = ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES,'l_out_act')\n",
    "#uu = tf.get_variable(yy[1])\n",
    "\n",
    "# Test gradient function\n",
    "w = tf.placeholder(tf.float32, name='w');\n",
    "u = tf.placeholder(tf.float32, name='u');\n",
    "f = w*u\n",
    "g = tf.gradients(l_out_cri,sta_act_pl);\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_pl)\n",
    "#g1 = optimizer.compute_gradients(l_out_act,[ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES,'l_hidden_act'),ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES,'l_out_act')])\n",
    "#g1 = optimizer.compute_gradients(l_out_cri,sta_act_pl)\n",
    "\n",
    "\n",
    "    \n",
    "x_values = np.zeros((10,1))\n",
    "x_var = tf.Variable(x_values, name=\"xVariable\", dtype=tf.float32)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #sess.run(copy_criact_critic)\n",
    "    \n",
    "    state = env.reset()\n",
    "    action_totake1 = sess.run(fetches=l_out_act, feed_dict={states_pl: np.reshape(state,(1,n_states))})\n",
    "    print(state)\n",
    "    print(action_totake1)\n",
    "\n",
    "    print('-')\n",
    "\n",
    "# test forward pass of critic network\n",
    "    #state = env.reset()\n",
    "    action = (np.random.rand()-0.5)*2; # Uniform distribution between -1 and 1\n",
    "    action = action_totake1\n",
    "#with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    action_totake = sess.run(fetches=l_out_cri, feed_dict={sta_act_pl: np.column_stack((np.reshape(state,(1,n_states)),action))})\n",
    "    print(state)\n",
    "    #print(action)\n",
    "    print(action_totake)\n",
    "    print('-')\n",
    "    \n",
    "    action_totake = sess.run(fetches=cl_out_cri, feed_dict={states_pl: np.reshape(state,(1,n_states))})\n",
    "    print(state)\n",
    "    print(action_totake)\n",
    "    print('-------------------------------------------------------------------------')\n",
    "    \n",
    "\n",
    "\n",
    "# test forward pass of targetactor network\n",
    "    state = env.reset()\n",
    "#with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    value = sess.run(fetches=l_out_act_t, feed_dict={states_pl: np.reshape(state,(1,n_states))})\n",
    "    print(state)\n",
    "    print(value)\n",
    "    print('-')\n",
    "\n",
    "# test forward pass of target critic network\n",
    "    #state = env.reset()\n",
    "    #action = (np.random.rand()-0.5)*2; # Uniform distribution between -1 and 1\n",
    "#with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    value = sess.run(fetches=l_out_cri_t, feed_dict={sta_act_pl: np.column_stack((np.reshape(state,(1,n_states)),action))})\n",
    "    print(state)\n",
    "    #print(action)\n",
    "    print(value)\n",
    "    print('-')\n",
    "    \n",
    "\n",
    "  #  print(x_var.eval())\n",
    "    aa = ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES,'l_out_cri')\n",
    "    bb = ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES,'l_hidden_cri')\n",
    "    #print(aa)\n",
    "    #print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mul:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"l_hidden_cri/Relu:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define Q-value loss function\n",
    "loss_f = tf.reduce_mean(tf.square(targets_pl-l_out_cri))\n",
    "#loss_f = (tf.square(targets_pl-l_out_cri))\n",
    "\n",
    "#Regularization\n",
    "#reg_scale = 0.001\n",
    "#regularize = tf.contrib.layers.l2_regularizer(reg_scale)\n",
    "#params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'l_out_cri')\n",
    "#reg_term = sum([regularize(param) for param in params])\n",
    "#loss_f += reg_term\n",
    "\n",
    "# Maximize the reward (maximize the output of the critic network)\n",
    "in_to_cri = l_out_act\n",
    "#print([states_pl, in_to_cri])\n",
    "print(in_to_cri)\n",
    "print(l_hidden_cri)\n",
    "#exReward = l_out_cri(feed_dict={sta_act_pl: [states_pl, in_to_cri]})\n",
    "\n",
    "\n",
    "# Gradient of the critic network wrt to the action\n",
    "#g_C = tf.gradients(l_out_cri,sta_act_pl)\n",
    "#print(g_C)\n",
    "\n",
    "# Gradient of the actor network wrt its own parameters\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_pl)\n",
    "#g_A = optimizer.compute_gradients(l_out_act,[tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'l_hidden_act'),tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'l_out_act')])\n",
    "#print(g_A)\n",
    "\n",
    "#Minus sign due to converting a maximization to a minimization problem\n",
    "sign = -1\n",
    "loss_c = tf.reduce_mean(tf.scalar_mul(sign,cl_out_cri))\n",
    "g_c = optimizer.compute_gradients(loss_c,[tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'l_hidden_act'),tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'l_out_act')])\n",
    "train_c = optimizer.apply_gradients(g_c)\n",
    "#train_c = optimizer.minimize(cl_out_cri,aa)\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_pl)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_pl)\n",
    "train_f = optimizer.minimize(loss_f)\n",
    "\n",
    "saver = tf.train.Saver() # we use this later to save the model\n",
    "print(loss_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Additional functions\n",
    "\n",
    "# Update the target network's parameters\n",
    "tau = 0.5\n",
    "def assign_trainables(t_p, p, tau=1.0):\n",
    "    \"\"\"Update trainable variables with rate tau.\"\"\"\n",
    "    obs = []\n",
    "    for i, t in enumerate(t_p):\n",
    "        for k in range(len(t)):\n",
    "            obs.append(t[k].assign((1-tau) * t[k].value() + tau * p[i][k].value()))\n",
    "    return obs\n",
    "\n",
    "\n",
    "# Get the target actor parameters\n",
    "pa_t_h = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='tl_hidden_act') #The hidden layer\n",
    "pa_t_o = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='tl_out_act')    #The output layer\n",
    "pa_t = [pa_t_h,pa_t_o]\n",
    "\n",
    "# Get the actor parameters\n",
    "pa_h = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='l_hidden_act') #The hidden layer\n",
    "pa_o = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='l_out_act')    #The output layer\n",
    "pa = [pa_h,pa_o];\n",
    "\n",
    "# Get the target critic parameters\n",
    "pc_t_h = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='tl_hidden_cri') #The hidden layer\n",
    "pc_t_o = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='tl_out_cri')    #The output layer\n",
    "pc_t = [pc_t_h,pc_t_o]\n",
    "\n",
    "# Get the critic parameters\n",
    "pc_h = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='l_hidden_cri') #The hidden layer\n",
    "pc_o = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='l_out_cri')    #The output layer\n",
    "pc = [pc_h,pc_o];\n",
    "\n",
    "# Get the critic-actor parameters\n",
    "p_h_c = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='cl_hidden_cri') #The hidden layer\n",
    "p_o_c = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='cl_out_cri')    #The output layer\n",
    "p = [p_h_c,p_o_c];\n",
    "\n",
    "update_target_actor = assign_trainables( pa_t,pa,tau)\n",
    "update_target_critic = assign_trainables( pc_t,pc,tau)\n",
    "copy_target_actor = assign_trainables( pa_t,pa,tau=1) #copy with tau=1 (clever trick)\n",
    "copy_target_critic = assign_trainables( pc_t,pc,tau=1)\n",
    "copy_critic_criact = assign_trainables( pc,p,tau=1)\n",
    "copy_criact_critic = assign_trainables( p,pc,tau=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Noise is changed\n",
      "0.05\n",
      "Episode:    0. Mean training reward: -103.68. Validation reward: -1081.85. Mean loss target:  31.82. Mean loss:  35.31.\n",
      "Episode:   10. Mean training reward: -1370.99. Validation reward: -940.75. Mean loss target:  69.97. Mean loss: 108.99.\n",
      "Episode:   20. Mean training reward: -1085.05. Validation reward: -754.26. Mean loss target:  65.44. Mean loss: 106.08.\n",
      "Episode:   30. Mean training reward: -879.14. Validation reward: -1011.64. Mean loss target:  57.31. Mean loss: 100.08.\n",
      "Episode:   40. Mean training reward: -762.43. Validation reward: -815.74. Mean loss target:  52.16. Mean loss:  96.61.\n",
      "Episode:   50. Mean training reward: -856.78. Validation reward: -519.47. Mean loss target:  49.17. Mean loss:  93.86.\n",
      "Episode:   60. Mean training reward: -732.70. Validation reward: -968.82. Mean loss target:  46.89. Mean loss:  91.53.\n",
      "Episode:   70. Mean training reward: -687.02. Validation reward: -1015.75. Mean loss target:  44.51. Mean loss:  89.46.\n",
      "Episode:   80. Mean training reward: -831.95. Validation reward: -1044.28. Mean loss target:  42.46. Mean loss:  87.52.\n",
      "Episode:   90. Mean training reward: -755.77. Validation reward: -760.63. Mean loss target:  40.92. Mean loss:  85.84.\n",
      "Episode:  100. Mean training reward: -748.12. Validation reward: -1212.13. Mean loss target:  40.00. Mean loss:  84.39.\n",
      "Episode:  110. Mean training reward: -833.52. Validation reward: -657.22. Mean loss target:  39.00. Mean loss:  83.17.\n",
      "Episode:  120. Mean training reward: -695.12. Validation reward: -636.35. Mean loss target:  37.90. Mean loss:  82.06.\n",
      "Episode:  130. Mean training reward: -625.92. Validation reward: -768.61. Mean loss target:  36.90. Mean loss:  80.97.\n",
      "Episode:  140. Mean training reward: -545.56. Validation reward: -517.42. Mean loss target:  35.84. Mean loss:  79.88.\n",
      "Episode:  150. Mean training reward: -493.78. Validation reward: -525.71. Mean loss target:  34.98. Mean loss:  78.73.\n",
      "Episode:  160. Mean training reward: -420.96. Validation reward: -575.59. Mean loss target:  34.01. Mean loss:  77.51.\n",
      "Episode:  170. Mean training reward: -248.53. Validation reward: -133.46. Mean loss target:  33.17. Mean loss:  76.23.\n",
      "Episode:  180. Mean training reward: -308.62. Validation reward: -1085.80. Mean loss target:  32.31. Mean loss:  75.01.\n",
      "Episode:  190. Mean training reward: -439.83. Validation reward: -128.31. Mean loss target:  31.65. Mean loss:  73.74.\n",
      "Episode:  200. Mean training reward: -396.40. Validation reward: -129.64. Mean loss target:  31.02. Mean loss:  72.52.\n",
      "Episode:  210. Mean training reward: -228.80. Validation reward: -798.83. Mean loss target:  30.47. Mean loss:  71.35.\n",
      "Episode:  220. Mean training reward: -193.34. Validation reward: -267.14. Mean loss target:  29.92. Mean loss:  70.19.\n",
      "Episode:  230. Mean training reward: -335.32. Validation reward: -1073.94. Mean loss target:  29.39. Mean loss:  69.04.\n",
      "Episode:  240. Mean training reward: -310.25. Validation reward: -352.82. Mean loss target:  28.82. Mean loss:  67.98.\n",
      "Episode:  250. Mean training reward: -196.14. Validation reward: -11.54. Mean loss target:  28.37. Mean loss:  66.93.\n",
      "Episode:  260. Mean training reward: -277.26. Validation reward: -720.51. Mean loss target:  27.87. Mean loss:  65.92.\n",
      "Episode:  270. Mean training reward: -645.81. Validation reward: -131.98. Mean loss target:  27.44. Mean loss:  64.91.\n",
      "Episode:  280. Mean training reward: -302.66. Validation reward: -633.71. Mean loss target:  27.07. Mean loss:  64.03.\n",
      "Episode:  290. Mean training reward: -239.60. Validation reward: -466.20. Mean loss target:  26.67. Mean loss:  63.16.\n",
      "Episode:  300. Mean training reward: -312.87. Validation reward: -520.75. Mean loss target:  26.27. Mean loss:  62.31.\n",
      "Episode:  310. Mean training reward: -364.77. Validation reward: -411.70. Mean loss target:  25.84. Mean loss:  61.49.\n",
      "Episode:  320. Mean training reward: -457.17. Validation reward: -645.61. Mean loss target:  25.42. Mean loss:  60.62.\n",
      "Episode:  330. Mean training reward: -328.73. Validation reward: -524.30. Mean loss target:  25.04. Mean loss:  59.78.\n",
      "Episode:  340. Mean training reward: -440.03. Validation reward: -393.44. Mean loss target:  24.63. Mean loss:  59.00.\n",
      "Episode:  350. Mean training reward: -454.10. Validation reward: -362.93. Mean loss target:  24.24. Mean loss:  58.27.\n",
      "Episode:  360. Mean training reward: -296.42. Validation reward: -125.75. Mean loss target:  23.90. Mean loss:  57.52.\n",
      "Episode:  370. Mean training reward: -181.07. Validation reward: -123.01. Mean loss target:  23.59. Mean loss:  56.82.\n",
      "Episode:  380. Mean training reward: -307.37. Validation reward: -628.36. Mean loss target:  23.24. Mean loss:  56.10.\n",
      "Episode:  390. Mean training reward: -244.06. Validation reward: -757.78. Mean loss target:  22.92. Mean loss:  55.37.\n",
      "Episode:  400. Mean training reward: -337.94. Validation reward: -129.02. Mean loss target:  22.66. Mean loss:  54.63.\n",
      "Episode:  410. Mean training reward: -235.68. Validation reward: -127.93. Mean loss target:  22.39. Mean loss:  53.94.\n",
      "Episode:  420. Mean training reward: -411.97. Validation reward: -619.55. Mean loss target:  22.20. Mean loss:  53.19.\n",
      "Episode:  430. Mean training reward: -375.11. Validation reward: -652.50. Mean loss target:  21.97. Mean loss:  52.45.\n",
      "Episode:  440. Mean training reward: -383.88. Validation reward: -263.64. Mean loss target:  21.73. Mean loss:  51.74.\n",
      "Episode:  450. Mean training reward: -327.06. Validation reward: -657.96. Mean loss target:  21.52. Mean loss:  51.05.\n",
      "Episode:  460. Mean training reward: -365.50. Validation reward: -757.19. Mean loss target:  21.34. Mean loss:  50.36.\n",
      "Episode:  470. Mean training reward: -465.24. Validation reward: -737.49. Mean loss target:  21.18. Mean loss:  49.68.\n",
      "Episode:  480. Mean training reward: -341.40. Validation reward: -118.69. Mean loss target:  20.98. Mean loss:  49.02.\n",
      "Episode:  490. Mean training reward: -481.61. Validation reward: -125.92. Mean loss target:  20.82. Mean loss:  48.36.\n",
      "Episode:  500. Mean training reward: -368.45. Validation reward: -250.11. Mean loss target:  20.70. Mean loss:  47.69.\n",
      "Episode:  510. Mean training reward: -341.23. Validation reward: -243.56. Mean loss target:  20.56. Mean loss:  47.02.\n",
      "Episode:  520. Mean training reward: -328.95. Validation reward: -253.82. Mean loss target:  20.42. Mean loss:  46.35.\n",
      "Episode:  530. Mean training reward: -701.48. Validation reward: -291.99. Mean loss target:  20.33. Mean loss:  45.67.\n",
      "Episode:  540. Mean training reward: -756.34. Validation reward: -1027.06. Mean loss target:  20.31. Mean loss:  45.02.\n",
      "Episode:  550. Mean training reward: -525.09. Validation reward: -124.87. Mean loss target:  20.21. Mean loss:  44.45.\n",
      "Episode:  560. Mean training reward: -186.50. Validation reward: -120.96. Mean loss target:  20.02. Mean loss:  43.94.\n",
      "Episode:  570. Mean training reward: -398.67. Validation reward: -1161.74. Mean loss target:  19.92. Mean loss:  43.32.\n",
      "Episode:  580. Mean training reward: -525.28. Validation reward: -236.35. Mean loss target:  19.91. Mean loss:  42.88.\n",
      "Episode:  590. Mean training reward: -493.78. Validation reward: -242.47. Mean loss target:  19.77. Mean loss:  42.34.\n",
      "Episode:  600. Mean training reward: -301.23. Validation reward: -131.21. Mean loss target:  19.63. Mean loss:  41.81.\n",
      "Episode:  610. Mean training reward: -929.29. Validation reward: -1642.10. Mean loss target:  19.77. Mean loss:  41.28.\n",
      "Episode:  620. Mean training reward: -1401.69. Validation reward: -1244.32. Mean loss target:  19.74. Mean loss:  40.92.\n",
      "Episode:  630. Mean training reward: -1204.22. Validation reward: -1135.80. Mean loss target:  19.60. Mean loss:  40.57.\n",
      "Episode:  640. Mean training reward: -1025.90. Validation reward: -713.83. Mean loss target:  19.48. Mean loss:  40.35.\n",
      "Episode:  650. Mean training reward: -899.45. Validation reward: -864.03. Mean loss target:  19.37. Mean loss:  40.27.\n",
      "Episode:  660. Mean training reward: -702.85. Validation reward: -617.59. Mean loss target:  19.27. Mean loss:  40.21.\n",
      "Episode:  670. Mean training reward: -543.63. Validation reward: -252.28. Mean loss target:  19.16. Mean loss:  40.05.\n",
      "Episode:  680. Mean training reward: -354.05. Validation reward: -241.48. Mean loss target:  19.05. Mean loss:  39.86.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  690. Mean training reward: -309.91. Validation reward: -416.11. Mean loss target:  18.93. Mean loss:  39.68.\n",
      "Episode:  700. Mean training reward: -217.06. Validation reward: -119.87. Mean loss target:  18.79. Mean loss:  39.50.\n",
      "Episode:  710. Mean training reward: -233.46. Validation reward: -118.33. Mean loss target:  18.66. Mean loss:  39.30.\n",
      "Episode:  720. Mean training reward: -208.87. Validation reward: -566.68. Mean loss target:  18.54. Mean loss:  39.11.\n",
      "Episode:  730. Mean training reward: -220.10. Validation reward: -244.14. Mean loss target:  18.41. Mean loss:  38.92.\n",
      "Episode:  740. Mean training reward: -260.35. Validation reward: -130.34. Mean loss target:  18.31. Mean loss:  38.69.\n",
      "Episode:  750. Mean training reward: -182.75. Validation reward: -125.54. Mean loss target:  18.20. Mean loss:  38.48.\n",
      "Episode:  760. Mean training reward: -158.82. Validation reward:  -3.53. Mean loss target:  18.09. Mean loss:  38.30.\n",
      "Episode:  770. Mean training reward: -182.09. Validation reward: -122.27. Mean loss target:  17.95. Mean loss:  38.11.\n",
      "Episode:  780. Mean training reward: -196.09. Validation reward: -300.48. Mean loss target:  17.82. Mean loss:  37.92.\n",
      "Episode:  790. Mean training reward: -156.36. Validation reward:  -1.22. Mean loss target:  17.69. Mean loss:  37.74.\n",
      "Episode:  800. Mean training reward: -168.34. Validation reward: -123.74. Mean loss target:  17.57. Mean loss:  37.54.\n",
      "Episode:  810. Mean training reward: -194.55. Validation reward: -126.72. Mean loss target:  17.45. Mean loss:  37.33.\n",
      "Episode:  820. Mean training reward: -143.47. Validation reward: -231.89. Mean loss target:  17.34. Mean loss:  37.09.\n",
      "Episode:  830. Mean training reward: -112.17. Validation reward: -115.94. Mean loss target:  17.25. Mean loss:  36.85.\n",
      "Episode:  840. Mean training reward: -199.75. Validation reward: -125.72. Mean loss target:  17.15. Mean loss:  36.61.\n",
      "Episode:  850. Mean training reward: -157.36. Validation reward: -126.30. Mean loss target:  17.06. Mean loss:  36.34.\n",
      "Episode:  860. Mean training reward: -136.09. Validation reward: -114.74. Mean loss target:  16.96. Mean loss:  36.08.\n",
      "Episode:  870. Mean training reward: -244.39. Validation reward: -304.56. Mean loss target:  16.87. Mean loss:  35.81.\n",
      "Episode:  880. Mean training reward: -198.42. Validation reward: -332.16. Mean loss target:  16.77. Mean loss:  35.56.\n",
      "Episode:  890. Mean training reward: -190.22. Validation reward: -127.47. Mean loss target:  16.70. Mean loss:  35.27.\n",
      "Episode:  900. Mean training reward: -151.44. Validation reward:  -1.24. Mean loss target:  16.64. Mean loss:  34.99.\n",
      "Episode:  910. Mean training reward: -174.52. Validation reward: -130.50. Mean loss target:  16.56. Mean loss:  34.74.\n",
      "Episode:  920. Mean training reward: -172.46. Validation reward: -127.95. Mean loss target:  16.49. Mean loss:  34.44.\n",
      "Episode:  930. Mean training reward: -198.67. Validation reward: -135.00. Mean loss target:  16.43. Mean loss:  34.13.\n",
      "Episode:  940. Mean training reward: -449.41. Validation reward: -249.84. Mean loss target:  16.44. Mean loss:  33.80.\n",
      "Episode:  950. Mean training reward: -528.66. Validation reward: -1362.73. Mean loss target:  16.52. Mean loss:  33.45.\n",
      "Episode:  960. Mean training reward: -846.54. Validation reward: -118.71. Mean loss target:  16.65. Mean loss:  33.00.\n",
      "Episode:  970. Mean training reward: -567.79. Validation reward: -621.32. Mean loss target:  16.74. Mean loss:  32.57.\n",
      "Episode:  980. Mean training reward: -282.27. Validation reward: -1219.20. Mean loss target:  16.78. Mean loss:  32.19.\n",
      "Episode:  990. Mean training reward: -297.99. Validation reward: -128.51. Mean loss target:  16.85. Mean loss:  31.77.\n",
      "Noise is changed\n",
      "0.025\n",
      "Episode: 1000. Mean training reward: -307.19. Validation reward: -267.70. Mean loss target:  16.92. Mean loss:  31.35.\n",
      "Episode: 1010. Mean training reward: -318.47. Validation reward: -242.90. Mean loss target:  16.98. Mean loss:  30.93.\n",
      "Episode: 1020. Mean training reward: -381.78. Validation reward: -880.91. Mean loss target:  17.05. Mean loss:  30.49.\n",
      "Episode: 1030. Mean training reward: -189.15. Validation reward: -263.17. Mean loss target:  17.08. Mean loss:  30.12.\n",
      "Episode: 1040. Mean training reward: -273.66. Validation reward: -235.34. Mean loss target:  17.08. Mean loss:  29.77.\n",
      "Episode: 1050. Mean training reward: -211.00. Validation reward: -126.76. Mean loss target:  17.11. Mean loss:  29.38.\n",
      "Episode: 1060. Mean training reward: -373.00. Validation reward: -888.77. Mean loss target:  17.11. Mean loss:  29.04.\n",
      "Episode: 1070. Mean training reward: -577.00. Validation reward: -125.15. Mean loss target:  17.26. Mean loss:  28.57.\n",
      "Episode: 1080. Mean training reward: -337.59. Validation reward: -343.61. Mean loss target:  17.38. Mean loss:  28.12.\n",
      "Episode: 1090. Mean training reward: -567.35. Validation reward: -814.57. Mean loss target:  17.51. Mean loss:  27.66.\n",
      "Episode: 1100. Mean training reward: -366.27. Validation reward: -246.48. Mean loss target:  17.61. Mean loss:  27.24.\n",
      "Episode: 1110. Mean training reward: -397.76. Validation reward: -128.19. Mean loss target:  17.69. Mean loss:  26.86.\n",
      "Episode: 1120. Mean training reward: -480.90. Validation reward: -1129.86. Mean loss target:  17.76. Mean loss:  26.43.\n",
      "Episode: 1130. Mean training reward: -437.85. Validation reward: -246.69. Mean loss target:  17.89. Mean loss:  25.97.\n",
      "Episode: 1140. Mean training reward: -658.89. Validation reward: -1015.94. Mean loss target:  17.98. Mean loss:  25.51.\n",
      "Episode: 1150. Mean training reward: -842.65. Validation reward: -1020.91. Mean loss target:  18.10. Mean loss:  25.06.\n",
      "Episode: 1160. Mean training reward: -902.31. Validation reward: -1469.46. Mean loss target:  18.23. Mean loss:  24.60.\n",
      "Episode: 1170. Mean training reward: -802.57. Validation reward: -1076.52. Mean loss target:  18.34. Mean loss:  24.23.\n",
      "Episode: 1180. Mean training reward: -723.21. Validation reward: -243.99. Mean loss target:  18.39. Mean loss:  23.93.\n",
      "Episode: 1190. Mean training reward: -420.76. Validation reward: -492.53. Mean loss target:  18.38. Mean loss:  23.69.\n",
      "Episode: 1200. Mean training reward: -430.16. Validation reward: -248.66. Mean loss target:  18.37. Mean loss:  23.48.\n",
      "Episode: 1210. Mean training reward: -267.55. Validation reward: -1099.27. Mean loss target:  18.34. Mean loss:  23.29.\n",
      "Episode: 1220. Mean training reward: -267.52. Validation reward: -121.29. Mean loss target:  18.32. Mean loss:  23.10.\n",
      "Episode: 1230. Mean training reward: -348.08. Validation reward: -273.13. Mean loss target:  18.29. Mean loss:  22.92.\n",
      "Episode: 1240. Mean training reward: -570.75. Validation reward: -125.98. Mean loss target:  18.31. Mean loss:  22.70.\n",
      "Episode: 1250. Mean training reward: -586.46. Validation reward: -1059.32. Mean loss target:  18.32. Mean loss:  22.49.\n",
      "Episode: 1260. Mean training reward: -253.24. Validation reward: -121.20. Mean loss target:  18.32. Mean loss:  22.29.\n",
      "Episode: 1270. Mean training reward: -242.87. Validation reward:  -0.59. Mean loss target:  18.26. Mean loss:  22.16.\n",
      "Episode: 1280. Mean training reward: -275.66. Validation reward: -525.46. Mean loss target:  18.19. Mean loss:  22.05.\n",
      "Episode: 1290. Mean training reward: -467.77. Validation reward: -119.23. Mean loss target:  18.20. Mean loss:  21.86.\n",
      "Episode: 1300. Mean training reward: -260.53. Validation reward: -121.67. Mean loss target:  18.20. Mean loss:  21.66.\n",
      "Episode: 1310. Mean training reward: -362.13. Validation reward: -228.84. Mean loss target:  18.21. Mean loss:  21.48.\n",
      "Episode: 1320. Mean training reward: -228.13. Validation reward: -121.66. Mean loss target:  18.19. Mean loss:  21.32.\n",
      "Episode: 1330. Mean training reward: -247.16. Validation reward: -513.33. Mean loss target:  18.20. Mean loss:  21.16.\n",
      "Episode: 1340. Mean training reward: -251.36. Validation reward: -124.21. Mean loss target:  18.19. Mean loss:  21.01.\n",
      "Episode: 1350. Mean training reward: -154.23. Validation reward: -250.42. Mean loss target:  18.13. Mean loss:  20.92.\n",
      "Episode: 1360. Mean training reward: -186.68. Validation reward: -491.03. Mean loss target:  18.07. Mean loss:  20.82.\n",
      "Episode: 1370. Mean training reward: -369.49. Validation reward: -120.88. Mean loss target:  18.05. Mean loss:  20.70.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1380. Mean training reward: -238.47. Validation reward: -255.30. Mean loss target:  17.99. Mean loss:  20.61.\n",
      "Episode: 1390. Mean training reward: -241.71. Validation reward: -128.07. Mean loss target:  17.93. Mean loss:  20.52.\n",
      "Episode: 1400. Mean training reward: -374.94. Validation reward: -635.27. Mean loss target:  17.89. Mean loss:  20.41.\n",
      "Episode: 1410. Mean training reward: -346.96. Validation reward: -244.33. Mean loss target:  17.85. Mean loss:  20.30.\n",
      "Episode: 1420. Mean training reward: -254.35. Validation reward: -859.80. Mean loss target:  17.80. Mean loss:  20.22.\n",
      "Episode: 1430. Mean training reward: -447.93. Validation reward: -392.56. Mean loss target:  17.77. Mean loss:  20.11.\n",
      "Episode: 1440. Mean training reward: -292.49. Validation reward: -119.85. Mean loss target:  17.72. Mean loss:  20.05.\n",
      "Episode: 1450. Mean training reward: -229.26. Validation reward: -245.59. Mean loss target:  17.67. Mean loss:  19.98.\n",
      "Episode: 1460. Mean training reward: -275.97. Validation reward: -125.65. Mean loss target:  17.60. Mean loss:  19.90.\n",
      "Episode: 1470. Mean training reward: -316.65. Validation reward: -128.68. Mean loss target:  17.55. Mean loss:  19.82.\n",
      "Episode: 1480. Mean training reward: -171.02. Validation reward: -259.13. Mean loss target:  17.50. Mean loss:  19.73.\n",
      "Episode: 1490. Mean training reward: -313.30. Validation reward: -883.04. Mean loss target:  17.44. Mean loss:  19.63.\n",
      "Episode: 1500. Mean training reward: -260.07. Validation reward: -501.46. Mean loss target:  17.40. Mean loss:  19.53.\n",
      "Episode: 1510. Mean training reward: -275.64. Validation reward: -750.87. Mean loss target:  17.36. Mean loss:  19.42.\n",
      "Episode: 1520. Mean training reward: -198.58. Validation reward: -251.22. Mean loss target:  17.32. Mean loss:  19.31.\n",
      "Episode: 1530. Mean training reward: -331.54. Validation reward: -122.91. Mean loss target:  17.29. Mean loss:  19.18.\n",
      "Episode: 1540. Mean training reward: -344.11. Validation reward: -881.55. Mean loss target:  17.25. Mean loss:  19.05.\n",
      "Episode: 1550. Mean training reward: -266.22. Validation reward: -126.10. Mean loss target:  17.22. Mean loss:  18.95.\n",
      "Episode: 1560. Mean training reward: -206.29. Validation reward: -121.92. Mean loss target:  17.18. Mean loss:  18.83.\n",
      "Episode: 1570. Mean training reward: -293.45. Validation reward: -121.79. Mean loss target:  17.15. Mean loss:  18.71.\n",
      "Episode: 1580. Mean training reward: -214.95. Validation reward: -122.48. Mean loss target:  17.10. Mean loss:  18.60.\n",
      "Episode: 1590. Mean training reward: -233.45. Validation reward: -247.15. Mean loss target:  17.06. Mean loss:  18.49.\n",
      "Episode: 1600. Mean training reward: -326.42. Validation reward: -255.01. Mean loss target:  17.01. Mean loss:  18.38.\n",
      "Episode: 1610. Mean training reward: -191.92. Validation reward: -377.27. Mean loss target:  16.96. Mean loss:  18.28.\n",
      "Episode: 1620. Mean training reward: -175.37. Validation reward: -129.98. Mean loss target:  16.89. Mean loss:  18.24.\n",
      "Episode: 1630. Mean training reward: -198.94. Validation reward: -243.21. Mean loss target:  16.82. Mean loss:  18.19.\n",
      "Episode: 1640. Mean training reward: -192.11. Validation reward: -127.87. Mean loss target:  16.74. Mean loss:  18.13.\n",
      "Episode: 1650. Mean training reward: -130.39. Validation reward: -130.13. Mean loss target:  16.67. Mean loss:  18.08.\n",
      "Episode: 1660. Mean training reward: -159.74. Validation reward: -762.56. Mean loss target:  16.59. Mean loss:  18.03.\n",
      "Episode: 1670. Mean training reward: -693.97. Validation reward: -494.10. Mean loss target:  16.59. Mean loss:  17.85.\n",
      "Episode: 1680. Mean training reward: -632.33. Validation reward: -1164.94. Mean loss target:  16.57. Mean loss:  17.77.\n",
      "Episode: 1690. Mean training reward: -487.03. Validation reward: -1019.42. Mean loss target:  16.55. Mean loss:  17.60.\n",
      "Episode: 1700. Mean training reward: -338.17. Validation reward: -127.32. Mean loss target:  16.49. Mean loss:  17.48.\n",
      "Episode: 1710. Mean training reward: -151.45. Validation reward: -131.50. Mean loss target:  16.42. Mean loss:  17.42.\n",
      "Episode: 1720. Mean training reward: -155.16. Validation reward: -116.89. Mean loss target:  16.35. Mean loss:  17.36.\n"
     ]
    }
   ],
   "source": [
    "# Inverted pendulum problem.\n",
    "\n",
    "# training settings\n",
    "noise_size = 0.1\n",
    "learning_rate = 0.001# you know this by now\n",
    "episodes = 10000 #Number of episodes (Outer loop)\n",
    "maxTime = 200 #Maximum time for an episode (Inner loop)\n",
    "epsilon = 0.1 #The epsilon in the epsilon-greedy method for exploration\n",
    "gamma = 0.95 #Discount factor\n",
    "valid_episode = 10 #Compute validation at every valid_episode'th episode\n",
    "ep = 0 #Episode counter for valid_episode\n",
    "Nd = 100000 #Size of D matrix for experience replay\n",
    "Nb = 64 #Size of minibatch in the stochastic gradient descent with experience replay\n",
    "ex_rep_counter = 1\n",
    "try:\n",
    "    statistics = []\n",
    "    r_stat = np.zeros((maxTime,valid_episode))\n",
    "    loss_stat = []\n",
    "    loss2_stat = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(copy_target_actor)\n",
    "        sess.run(copy_target_critic)\n",
    "        sess.run(copy_criact_critic)\n",
    "        print('start training')\n",
    "        D = np.zeros((Nd,n_states*2+1+1+1)) #5 is for (aj,xj,rj,xj+1,done). It's the history matrix for experience replay\n",
    "        D_counter = 0 #Counter in circular D-buffer\n",
    "        for episode in range(episodes):\n",
    "            x = env.reset()\n",
    "            a_noise = 0\n",
    "            for timestep in range(maxTime):\n",
    "                # Action noise strategy (at least respect the bounds on the action)\n",
    "                #------------------------------------------------------------------\n",
    "                at = sess.run(fetches=l_out_act, feed_dict={states_pl: np.reshape(x,(1,n_states))}) #Take action according to current best policy\n",
    "                #print(at)\n",
    "                a_noise = a_noise*0.9 + noise_size*np.random.normal(0,1,1) #sample the exploration noise from a normal distribution (can be made more sophisticated)\n",
    "                at = at + a_noise\n",
    "                at = np.clip(at,-2,2)\n",
    "                #Apply action and observe reward and the new state\n",
    "                #-------------------------------------------------\n",
    "                \n",
    "                #print(at)\n",
    "                #print(x)\n",
    "                x_next, r, done, _ = env.step(at) #Apply the action\n",
    "\n",
    "                r_stat[timestep,ep] = r #Save for statistics\n",
    "                \n",
    "                #Save step into D-buffer for Experience replay\n",
    "                #---------------------------------------------\n",
    "                #Update into a a circular buffer\n",
    "                update_index_ER = D_counter\n",
    "                D[update_index_ER,:] = np.concatenate((np.reshape(x,n_states),np.reshape(at,n_actions),np.reshape(r,1),np.reshape(x_next,n_states),np.reshape(done,1)))\n",
    "                D_counter = D_counter + 1\n",
    "                if D_counter == Nd:\n",
    "                    D_counter = 0\n",
    "\n",
    "                #Sample the minibatch with Experience replay\n",
    "                #------------------------------------------------------------\n",
    "                y_mb = np.zeros((Nb,1))\n",
    "                at_mb = np.zeros((Nb,n_actions))\n",
    "                x_mb = np.zeros((Nb,n_states))\n",
    "                for batch_number in range(Nb):\n",
    "                    sample_index_ER = np.random.randint(0,ex_rep_counter)\n",
    "                    Drow= D[sample_index_ER,:]\n",
    "                    #Unpack the history quintuple\n",
    "                    xj = Drow[0:n_states]\n",
    "                    aj = Drow[n_states]\n",
    "                    rj = Drow[n_states+1]\n",
    "                    xj1 = Drow[n_states+2:2*n_states+2]\n",
    "                    done1 = Drow[-1]\n",
    "                    \n",
    "                    #Fill out the y-value (right hand side of the Bellman equation)\n",
    "                    if done1:  #If we have terminated the game\n",
    "                        y_mb[batch_number] = rj\n",
    "                    else: #If we have not terminated the game \n",
    "                        a_next = sess.run(fetches=l_out_act_t, feed_dict={states_pl: np.reshape(xj1,(1,n_states))})\n",
    "                        value_next = sess.run(fetches=l_out_cri_t, feed_dict={sta_act_pl: np.column_stack((np.reshape(xj1,(1,n_states)),np.reshape(a_next,(1,1))))})\n",
    "                        y_mb[batch_number,:] = rj+gamma*value_next\n",
    "                        \n",
    "                    #Fill out the action a_mb\n",
    "                    at_mb[batch_number] = aj\n",
    "                    \n",
    "                    #Fill out the state x_mb\n",
    "                    x_mb[batch_number,:] = xj\n",
    "                \n",
    "                \n",
    "                if ex_rep_counter != Nd: # Handle initial filling of experience replay\n",
    "                    ex_rep_counter = ex_rep_counter + 1\n",
    "                #Do a stochastic gradient descent on the critic network\n",
    "                #------------------------------------------------------\n",
    "                loss,_ = sess.run(fetches=[loss_f,train_f], feed_dict={\n",
    "                    targets_pl: y_mb,\n",
    "                    sta_act_pl: np.column_stack((x_mb,at_mb)),\n",
    "                    learning_rate_pl: learning_rate\n",
    "                })    \n",
    "                \n",
    "                loss_stat.append(loss) #Save for statistics\n",
    "                sess.run(copy_criact_critic)\n",
    "                # Compute the gradients for the actor network (with same mini-batches)\n",
    "                #---------------------------------------------------------------------\n",
    "                \n",
    "                #print(pa_h[0].eval())\n",
    "                loss2,_,gc = sess.run(fetches=[loss_c,train_c,g_c], feed_dict={\n",
    "                    states_pl: x_mb,\n",
    "                    learning_rate_pl: learning_rate\n",
    "                })  \n",
    "                loss2_stat.append(loss2) #Save for statistics\n",
    "                #print('-')\n",
    "                #print(gc)\n",
    "                #print(pa_h[0].eval())\n",
    "                #print(sess.run(cl_out_cri,feed_dict={states_pl: x_mb}))\n",
    "                #print('-')\n",
    "\n",
    "\n",
    "                \n",
    "                # Update target networks\n",
    "                #------------------------------\n",
    "                ops = sess.run(update_target_actor)  #Update the actor target network\n",
    "                ops = sess.run(update_target_critic) #Update the critic target network\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                #Prepare for next timestep\n",
    "                #----------------------------\n",
    "                \n",
    "                x = x_next\n",
    "                if done: \n",
    "                    #print('Done before time!!!')\n",
    "                    break #Game over! And end the episode\n",
    "                \n",
    "             #Validation time!\n",
    "            #----------------\n",
    "            if (episode % 1000) == 0:\n",
    "                noise_size = noise_size/2\n",
    "                print('Noise is changed')\n",
    "                print(noise_size)\n",
    "            \n",
    "            if (episode % valid_episode) == 0: \n",
    "                #Computation of validation by playing the game once\n",
    "                #--------------------------------------------------\n",
    "                x = env.reset()\n",
    "                r_valid = np.zeros(maxTime)\n",
    "                #print(x)\n",
    "                for time_valid in range(maxTime):\n",
    "                    #Only pick greedy action in validation!\n",
    "                    action = sess.run(fetches=l_out_act, feed_dict={states_pl: [x]})\n",
    "                    at = action\n",
    "                \n",
    "                    x_next, r, done, _ = env.step(at) #Apply the action\n",
    "                    r_valid[time_valid] = r #Save for statistics\n",
    "                    if done: break #Game over! And end the episode\n",
    "                    x = np.reshape(x_next,(n_states,))\n",
    "                \n",
    "                \n",
    "                #Print out validation statistics\n",
    "                #-------------------------------\n",
    "                r_stat_mean = np.mean(np.sum(r_stat,axis=0))\n",
    "                r_valid_sum = np.sum(r_valid)\n",
    "                loss_stat_mean = np.mean(loss_stat,axis=0)\n",
    "                loss2_stat_mean = np.mean(loss2_stat,axis=0)\n",
    "                print('Episode: %4d. Mean training reward: %6.2f. Validation reward: %6.2f. Mean loss target: %6.2f. Mean loss: %6.2f.' % (episode, r_stat_mean, r_valid_sum, loss_stat_mean, loss2_stat_mean))\n",
    "                #Reset statistics variables\n",
    "                #--------------------------\n",
    "                ep = 0 #Reset episode counter between validations\n",
    "                r_stat = np.zeros((maxTime,valid_episode)) \n",
    "                #saver.save(sess, 'tmp2/model.ckpt')\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                ep = ep + 1\n",
    "                \n",
    "        print('done')\n",
    "        # save session\n",
    "        saver.save(sess, 'tmp2/model.ckpt')\n",
    "except KeyboardInterrupt:\n",
    "    pass            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-03 22:02:50,274] Restoring parameters from tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.45161094  0.89221497  0.24255418]\n",
      "[-250.42607117]\n"
     ]
    }
   ],
   "source": [
    "# review solution\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"tmp/model.ckpt\")\n",
    "    x = env.reset()\n",
    "    print(x)\n",
    "    view = Viewer(env, custom_render=True)\n",
    "    r_sum = 0\n",
    "    for _ in range(100):\n",
    "        env.render() # uncomment this to use gym's own render function\n",
    "        #a = get_action(sess, s, stochastic=False)\n",
    "        a = sess.run(fetches=l_out_act, feed_dict={states_pl: np.reshape(x,(1,n_states))})\n",
    "        #print(a)\n",
    "        #value = sess.run(fetches=l_out, feed_dict={states_pl: [x],is_training_pl: False})\n",
    "        #a=value.argmax()\n",
    "        x, r, done, _ = env.step(a)\n",
    "        r_sum = r_sum + r\n",
    "    env.render(close=True) # uncomment this to use gym'm own render function\n",
    "    print(r_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-03 22:02:47,734] Restoring parameters from tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.82435834]]\n",
      "[[-56.77286911]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"tmp/model.ckpt\")\n",
    "    x = [0,0,0]\n",
    "    a = sess.run(fetches=l_out_act, feed_dict={states_pl: np.reshape(x,(1,n_states))})\n",
    "    b = sess.run(fetches=l_out_cri, feed_dict={sta_act_pl: np.column_stack((np.reshape(x,(1,n_states)),np.reshape(a,(1,1))))})\n",
    "    print(a)\n",
    "    print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
