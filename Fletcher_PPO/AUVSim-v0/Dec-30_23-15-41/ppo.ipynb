{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Stuff for Python 2.7\n",
    "Please make sure you're running python 2.7\n",
    "\n",
    "Main Script for Running PPO Algorithm on AUVSim-v0\n",
    "\n",
    "Adapted from work by Patrick Coady (pat-coady.github.io)\n",
    "\n",
    "#### Author:  Fletcher Thompson\n",
    "\n",
    "#### see policy.py, value_function.py policy and value network objects, see utils.py for the logger and scaling objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fakegym\n",
    "import numpy as np\n",
    "from policy import Policy\n",
    "from value_function import NNValueFunction\n",
    "import scipy.signal\n",
    "from utils import Logger, Scaler\n",
    "from datetime import datetime\n",
    "import os\n",
    "import argparse\n",
    "import signal\n",
    "import tensorflow as tf\n",
    "from IPython.lib import backgroundjobs as bg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(x, gamma):\n",
    "    \"\"\" Calculate discounted forward sum of a sequence at each point \"\"\"\n",
    "    return scipy.signal.lfilter([1.0], [1.0, -gamma], x[::-1])[::-1]\n",
    "\n",
    "def add_disc_sum_rew(trajectories, gamma):\n",
    "    \"\"\" Adds discounted sum of rewards to all time steps of all trajectories\n",
    "\n",
    "    Args:\n",
    "        trajectories: as returned by run_policy()\n",
    "        gamma: discount\n",
    "\n",
    "    Returns:\n",
    "        None (mutates trajectories dictionary to add 'disc_sum_rew')\n",
    "    \"\"\"\n",
    "    for trajectory in trajectories:\n",
    "        if gamma < 0.999:  # don't scale for gamma ~= 1\n",
    "            rewards = trajectory['rewards'] * (1 - gamma)\n",
    "        else:\n",
    "            rewards = trajectory['rewards']\n",
    "        disc_sum_rew = discount(rewards, gamma)\n",
    "        trajectory['disc_sum_rew'] = disc_sum_rew\n",
    "\n",
    "def add_value(sess, trajectories, val_func):\n",
    "    \"\"\" Adds estimated value to all time steps of all trajectories\n",
    "\n",
    "    Args:\n",
    "        trajectories: as returned by run_policy()\n",
    "        val_func: object with predict() method, takes observations\n",
    "            and returns predicted state value\n",
    "\n",
    "    Returns:\n",
    "        None (mutates trajectories dictionary to add 'values')\n",
    "    \"\"\"\n",
    "    for trajectory in trajectories:\n",
    "        observes = trajectory['observes']\n",
    "        values = val_func.predict(sess,observes)\n",
    "        trajectory['values'] = values\n",
    "\n",
    "\n",
    "def add_gae(trajectories, gamma, lam):\n",
    "    \"\"\" Add generalized advantage estimator.\n",
    "    https://arxiv.org/pdf/1506.02438.pdf\n",
    "\n",
    "    Args:\n",
    "        trajectories: as returned by run_policy(), must include 'values'\n",
    "            key from add_value().\n",
    "        gamma: reward discount\n",
    "        lam: lambda (see paper).\n",
    "            lam=0 : use TD residuals\n",
    "            lam=1 : A =  Sum Discounted Rewards - V_hat(s)\n",
    "\n",
    "    Returns:\n",
    "        None (mutates trajectories dictionary to add 'advantages')\n",
    "    \"\"\"\n",
    "    for trajectory in trajectories:\n",
    "        if gamma < 0.999:  # don't scale for gamma ~= 1\n",
    "            rewards = trajectory['rewards'] * (1 - gamma)\n",
    "        else:\n",
    "            rewards = trajectory['rewards']\n",
    "        values = trajectory['values']\n",
    "        # temporal differences\n",
    "        tds = rewards - values + np.append(values[1:] * gamma, 0)\n",
    "        advantages = discount(tds, gamma * lam)\n",
    "        trajectory['advantages'] = advantages\n",
    "\n",
    "def build_train_set(trajectories):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        trajectories: trajectories after processing by add_disc_sum_rew(),\n",
    "            add_value(), and add_gae()\n",
    "\n",
    "    Returns: 4-tuple of NumPy arrays\n",
    "        observes: shape = (N, obs_dim)\n",
    "        actions: shape = (N, act_dim)\n",
    "        advantages: shape = (N,)\n",
    "        disc_sum_rew: shape = (N,)\n",
    "    \"\"\"\n",
    "    observes = np.concatenate([t['observes'] for t in trajectories])\n",
    "    actions = np.concatenate([t['actions'] for t in trajectories])\n",
    "    disc_sum_rew = np.concatenate([t['disc_sum_rew'] for t in trajectories])\n",
    "    advantages = np.concatenate([t['advantages'] for t in trajectories])\n",
    "    # normalize advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)\n",
    "    return observes, actions, advantages, disc_sum_rew\n",
    "\n",
    "def log_batch_stats(observes, actions, advantages, disc_sum_rew, logger, episode):\n",
    "    \"\"\" Log various batch statistics \"\"\"\n",
    "    logger.log({'_mean_obs': np.mean(observes),\n",
    "                '_min_obs': np.min(observes),\n",
    "                '_max_obs': np.max(observes),\n",
    "                '_std_obs': np.mean(np.var(observes, axis=0)),\n",
    "                '_mean_act': np.mean(actions),\n",
    "                '_min_act': np.min(actions),\n",
    "                '_max_act': np.max(actions),\n",
    "                '_std_act': np.mean(np.var(actions, axis=0)),\n",
    "                '_mean_adv': np.mean(advantages),\n",
    "                '_min_adv': np.min(advantages),\n",
    "                '_max_adv': np.max(advantages),\n",
    "                '_std_adv': np.var(advantages),\n",
    "                '_mean_discrew': np.mean(disc_sum_rew),\n",
    "                '_min_discrew': np.min(disc_sum_rew),\n",
    "                '_max_discrew': np.max(disc_sum_rew),\n",
    "                '_std_discrew': np.var(disc_sum_rew),\n",
    "                '_Episode': episode\n",
    "                })\n",
    "\n",
    "def run_episode(sess,env, policy, scaler):\n",
    "    \"\"\" Run single episode\n",
    "\n",
    "    Args:\n",
    "        env: ai gym environment\n",
    "        policy: policy object with sample() method\n",
    "        scaler: scaler object, used to scale/offset each observation dimension\n",
    "            to a similar range\n",
    "\n",
    "    Returns: 4-tuple of NumPy arrays\n",
    "        observes: shape = (episode len, obs_dim)\n",
    "        actions: shape = (episode len, act_dim)\n",
    "        rewards: shape = (episode len,)\n",
    "        unscaled_obs: useful for training scaler, shape = (episode len, obs_dim)\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    observes, actions, rewards, unscaled_obs = [], [], [], []\n",
    "    done = False\n",
    "    step = 0.0\n",
    "    scale, offset = scaler.get()\n",
    "    scale[-1] = 1.0  # don't scale time step feature\n",
    "    offset[-1] = 0.0  # don't offset time step feature\n",
    "    while not done:\n",
    "        obs = obs.astype(np.float32).reshape((1, -1))\n",
    "        obs = np.append(obs, [[step]], axis=1)  # add time step feature\n",
    "        unscaled_obs.append(obs)\n",
    "        obs = (obs - offset) * scale  # center and scale observations\n",
    "        observes.append(obs)\n",
    "        action = policy.sample(sess, obs).reshape((1, -1)).astype(np.float32)\n",
    "        actions.append(action)\n",
    "        obs, reward, done, _ = env.step(np.squeeze(action, axis=0))\n",
    "        if not isinstance(reward, float):\n",
    "            reward = np.asscalar(reward)\n",
    "        rewards.append(reward)\n",
    "        step += 5e-2  # increment time step feature\n",
    "\n",
    "    return (np.concatenate(observes), np.concatenate(actions),\n",
    "            np.array(rewards, dtype=np.float64), np.concatenate(unscaled_obs))\n",
    "\n",
    "\n",
    "def run_policy(sess, env, policy, scaler, logger, episodes):\n",
    "    \"\"\" Run policy and collect data for a minimum of min_steps and min_episodes\n",
    "\n",
    "    Args:\n",
    "        env: ai gym environment\n",
    "        policy: policy object with sample() method\n",
    "        scaler: scaler object, used to scale/offset each observation dimension\n",
    "            to a similar range\n",
    "        logger: logger object, used to save stats from episodes\n",
    "        episodes: total episodes to run\n",
    "\n",
    "    Returns: list of trajectory dictionaries, list length = number of episodes\n",
    "        'observes' : NumPy array of states from episode\n",
    "        'actions' : NumPy array of actions from episode\n",
    "        'rewards' : NumPy array of (un-discounted) rewards from episode\n",
    "        'unscaled_obs' : NumPy array of (un-discounted) rewards from episode\n",
    "    \"\"\"\n",
    "    print(\"Running Policy for {} Episodes\".format(episodes))\n",
    "    total_steps = 0\n",
    "    trajectories = []\n",
    "    for e in range(episodes):\n",
    "        observes, actions, rewards, unscaled_obs = run_episode(sess, env, policy, scaler)\n",
    "        total_steps += observes.shape[0]\n",
    "        trajectory = {'observes': observes,\n",
    "                      'actions': actions,\n",
    "                      'rewards': rewards,\n",
    "                      'unscaled_obs': unscaled_obs}\n",
    "        trajectories.append(trajectory)\n",
    "        print(\"Episode: {}\".format(e))\n",
    "    unscaled = np.concatenate([t['unscaled_obs'] for t in trajectories])\n",
    "    scaler.update(unscaled)  # update running statistics for scaling observations\n",
    "    logger.log({'_MeanReward': np.mean([t['rewards'].sum() for t in trajectories]),\n",
    "                'Steps': total_steps})\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop\n",
    "Modify hyperparameters and training stop conditions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET HYPERPARAMETERS ##\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "env_name=\"AUVSim-v0\"\n",
    "model_dir = \"tmp/model\"\n",
    "hid1_mult=10 # sets size of first hidden layer as a multiple of observation dimension size\n",
    "kl_targ=0.006 # sets the maximum allowable KL divergence for the policy update\n",
    "policy_logvar=-1.0 # sets the initial log variance of the policy\n",
    "num_episodes = 50000 # stopping condition (total number of episodes)\n",
    "batch_size = 20 # number of trajectories to generate for each training session\n",
    "gamma = 0.995 # discount factor for future reward summation\n",
    "lam = 0.995 # general advantage estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUN THE ENVIRONMENT AS A BACKGROUND TASK\n",
    "Necessary because ROS blocks .ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THE ENVIRONMENT ##\n",
    "env = fakegym.env()\n",
    "jobs = bg.BackgroundJobManager()\n",
    "# run the interface as a background process\n",
    "jobs.new('env.run()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim += 1  # add 1 to the obs dimension for time\n",
    "now = datetime.utcnow().strftime(\"%b-%d_%H:%M:%S\")  # create unique directories\n",
    "logger = Logger(logname=env_name, now=now)\n",
    "scaler = Scaler(obs_dim)\n",
    "\n",
    "## BUILD THE NETWORKS ##\n",
    "tf.reset_default_graph()\n",
    "G = tf.Graph()\n",
    "val_func = NNValueFunction(G,obs_dim, hid1_mult)\n",
    "policy = Policy(G, obs_dim, act_dim, kl_targ, hid1_mult, policy_logvar)\n",
    "\n",
    "## INITIALIZE THE NETWORK VARIABLES ##\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "with G.as_default():\n",
    "    sess = tf.Session(graph=G,config=config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "writer = tf.summary.FileWriter(\"./tmp/log\", G) # to visualise the graph through tensorboard\n",
    "print(\"***\\nINFO:  tensorboard visualisation\\nrun 'tensorboard --logdir=tmp' in another terminal (from this directory) to visualise the networks\\n***\")\n",
    "# run a few episodes of random policy for observation normalisation:\n",
    "print(\"Generating random states to initialise the observation scaler\")\n",
    "run_policy(sess,env, policy, scaler, logger, episodes=5)\n",
    "print(\"Beginning Training\")\n",
    "episode = 0\n",
    "while episode < num_episodes:\n",
    "    print(\"Episode {} out of {}\".format(episode,num_episodes))\n",
    "    trajectories = run_policy(sess,env, policy, scaler, logger, episodes=batch_size)\n",
    "    episode += len(trajectories)\n",
    "    add_value(sess, trajectories, val_func)  # add estimated values to episodes\n",
    "    add_disc_sum_rew(trajectories, gamma)  # calculated discounted sum of rewards\n",
    "    add_gae(trajectories, gamma, lam)  # calculate advantage\n",
    "    # concatenate all episodes into single NumPy arrays\n",
    "    observes, actions, advantages, disc_sum_rew = build_train_set(trajectories)\n",
    "    # add various stats to training log:\n",
    "    log_batch_stats(observes, actions, advantages, disc_sum_rew, logger, episode)\n",
    "    policy.update(sess,observes, actions, advantages, logger)  # update policy\n",
    "    val_func.fit(sess,observes, disc_sum_rew, logger)  # update value function\n",
    "    logger.write(display=True)  # write logger results to file and stdout\n",
    "    if episode % 1000 == 0:\n",
    "        saver.save(sess, model_dir, global_step=i) #save the model every 1000 episodes\n",
    "logger.close()\n",
    "sess.close()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
